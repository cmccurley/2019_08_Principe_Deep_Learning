@incollection{Liu2010,
address = {New Jersey},
author = {Liu, Weifeng and Principe, Jose C. and Haykin, Simon},
booktitle = {Kernel Adaptive Filtering: A Comprehensive Introduction},
edition = {1},
pages = {1--26,27--68},
publisher = {John Wiley and Sons, Inc.},
title = {{Chapter 1- Background and Preview, Chapter 2- Kernel Least Mean Square Algorithm}},
year = {2010}
}
@article{Fancourt1997a,
author = {Fancourt, Craig L and Principe, Jose C},
pages = {1--51},
title = {{Exploiting Multi-Modality for Segmentation and Modeling of Non-Stationary Time Series}},
year = {1997}
}
@article{Zhu2014a,
abstract = {Onset detection is the foundation and key to high-level audio processing like music retrieval and transcription. Research shows that the detection algorithm is associated with instrument category, and high accuracy can be achieved in instrument recognition studies. Thus the adaptive detection system based on instrument recognition was proposed in this paper. The system uses HMM classifier to identify input audio falling into four categories, adaptively adopts suitable detection algorithm for each type, and output onset times in the end. The experiment results show that onset evaluation values, such as the F-measure value, have been improved in the system.},
author = {Zhu, Bing and Gan, Jiayue and Cai, Juanjuan and Wang, Yi and Wang, Hui},
doi = {10.1109/ICOSP.2014.7015428},
isbn = {978-1-4799-2186-7},
issn = {2164-5221},
journal = {International Conference on Signal Processing Proceedings, ICSP},
keywords = {HMM,Instrument recognition,Onset detection},
number = {October},
pages = {2416--2421},
title = {{Adaptive onset detection based on instrument recognition}},
volume = {2015-January},
year = {2014}
}
@article{Brognaux2016a,
abstract = {Speech segmentation refers to the problem of determining the phoneme boundaries from an acoustic recording of an utterance together with its orthographic transcription. This paper focuses on a particular case of Hidden Markov Model (HMM) based forced alignment in which the models are directly trained on the corpus to align. The obvious advantage of this technique is that it is applicable to any language or speaking style and does not require manually-aligned data. Through a systematic stepby- step study, the role played by various training parameters (e.g. models configuration, number of training iterations) on the alignment accuracy is assessed, with corpora varying in speaking style and language. Based on a detailed analysis of the errors commonly made by this technique, we also investigate the use of additional fully automatic strategies to improve the alignment. Beside the use of supplementary acoustic features, we explore two novel approaches: an initialization of the silence models based on a Voice Activity Detection (VAD) algorithm and the consideration of the forced alignment of the timereversed sound. The evaluation is carried out on 12 corpora of different sizes, languages (some being under-resourced) and speaking styles. It aims at providing a comprehensive study of the alignment accuracy achieved by the different versions of the speech segmentation algorithm depending on corpus-related specificities. While the baseline method is shown to reach good alignment rates with corpora as small as 2 minutes, we also emphasize the benefit of using a few seconds of bootstrapping data. Regarding improvement methods, our results show that the insertion of additional features outperforms both other strategies. The performance of VAD, however, is shown to be notably striking on very small corpora, correcting more than 60 {\%} of the errors superior to 40 ms. Finally, the combination of the three improvement methods is also pointed out as providing the highest alignment r- tes, with very low variability across the corpora, regardless of their size. This combined technique is shown to outperform available speaker-independent models, improving the alignment rate by 8 to 10 {\%} absolute.},
author = {Brognaux, Sandrine and Drugman, Thomas},
doi = {10.1109/TASLP.2015.2456421},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Corpora annotation,hidden Markov models,phonetic alignment,speech segmentation},
number = {1},
pages = {5--15},
title = {{HMM-Based Speech Segmentation: Improvements of Fully Automatic Approaches}},
volume = {24},
year = {2016}
}
@article{Jancovic2017a,
abstract = {This paper presents an automatic system for recognition of bird species from field audio recordings. The proposed system employs a novel method for detection of sinusoidal components in the acoustic scene. This provides a segmentation of the signal and also feature representation of each segment in terms of frequencies over time, referred to as frequency track. We employ hidden Markov models (HMMs) to model the temporal evolution of frequency tracks. We demonstrate the effect of including local temporal dynamics of frequency tracks and HMM modelling parameters. Experiments are performed on over 33 hours of field recordings, containing 30 bird species. Evaluations demonstrate that the HMM-based temporal modelling provides considerable performance improvement over a system based on Gaussian mixture modelling. The proposed HMM-based system is capable of recognising bird species with accuracy over 85{\%} from only 3 seconds of detected signal.},
author = {Jancovic, Peter and Kokuer, Munevver},
doi = {10.23919/EUSIPCO.2017.8081515},
isbn = {9780992862671},
issn = {15206149},
journal = {25th European Signal Processing Conference, EUSIPCO 2017},
keywords = {Bird species detection,Cohort,Element,Field recording,Frequency track,HMM,Hidden Markov model,Outlier,Score normalisation,Sinusoid detection,Sinusoidal modelling,Unsupervised training,Vocalisation},
pages = {1779--1783},
title = {{Automatic Detection of bird species from audio field recordings using HMM-based modelling of frequency tracks}},
volume = {2017-January},
year = {2017}
}
@article{Tiwari2010a,
abstract = {Speech processing is emerged as one of the important application area of digital signal processing. Various fields for research in speech processing are speech recognition, speaker recognition, speech synthesis, speech coding etc. The objective of automatic speaker recognition is to extract, characterize and recognize the information about speaker identity. Feature extraction is the first step for speaker recognition. Many algorithms are suggested/developed by the researchers for feature extraction. In this work, the Mel Frequency Cepstrum Coefficient (MFCC) feature has been used for designing a text dependent speaker identification system. Some modifications to the existing technique of MFCC for feature extraction are also suggested to improve the speaker recognition efficiency.},
author = {Tiwari, Vibha},
journal = {International Journal on Emerging Technologies},
keywords = {feature extraction,mel frequency cepstral coefficients,mfcc,speaker recognition},
number = {1},
pages = {19--22},
title = {{MFCC and its applications in speaker recognition}},
volume = {1},
year = {2010}
}
@article{Nijhawan2014a,
author = {Nijhawan, Geeta and Soni, M K},
journal = {International Journal on Recent Trends in Engineering and Technology},
number = {1},
pages = {211--218},
title = {{Speaker Recognition Using MFCC and Vector Quantisation}},
volume = {11},
year = {2014}
}
@article{Chauhan2014a,
abstract = {Speech processing is now an emerging technology of signal processing. Some research areas of speech processing are recognition of speech, speaker identification (SI), speech synthesis etc. Speaker identification is important research area of speech processing. SI means identifying the speaker based on his spoken speech. The main use of SI is to recognize the speech owner based on the speaking style of the speaker. SI is mainly used in forensic analysis, home control system, database access services etc. For SI two things are essential. One is feature extraction and another is feature matching. Feature extraction is extraction of small information from the available audio wave signal. That information can be used to represent the particular speaker. For SI, There are many feature extraction techniques like LPC (Linear Predictive Coefficients), MFCC (Mel Frequency Cepstral Coefficients), PLP (Perceptual Linear Predictive Coefficients) and many more are used. MFCC is one of them and it gives good (efficient) identification results. Factor affecting on SI is noise, sampling rate, number of frames etc., and among them noise is the most critical factor. We found that MFCC is not much effective in the noisy environment, especially when the noise condition mismatch. The identification rate becomes poor and poor when the noise level increases. To improve the performance of SI in a real world noisy environment, we propose a technique which is a variant of MFCC. Proposed MFCC includes wiener filter which is good for handling the noise in speech. In this paper, it is suggested that the wiener filter is effective in the frequency domain rather than the time domain based on our experiments. We got 88.57{\%} average identification rate with NOIZEUS database by our proposed technique. In feature matching, the unknown speech is classified by using some classifier. We have used neural network for feature matching.},
author = {Chauhan, Paresh M and Desai, Nikita P},
doi = {10.1109/ICGCCEE.2014.6921394},
isbn = {9781479949823},
journal = {Proceeding of the IEEE International Conference on Green Computing, Communication and Electrical Engineering, ICGCCEE 2014},
keywords = {Noise mismatch,Real world noisy environment,Speaker identification,Wiener filter},
title = {{Mel Frequency Cepstral Coefficients (MFCC) based speaker identification in noisy environment using wiener filter}},
year = {2014}
}
@article{Grama2017a,
author = {Grama, Lacrimioara and Rusu, Corneliu},
doi = {10.1109/ISPA.2017.8073600},
isbn = {9781509040117},
issn = {18492266},
journal = {International Symposium on Image and Signal Processing and Analysis, ISPA},
keywords = {Bayesian network,CHIRP,FLR,LMT,MFCC,MLP,Random Forests,SVM,audio classification,kNN,sinusoidal liftering},
number = {Ispa},
pages = {225--230},
title = {{Choosing an accurate number of mel frequency cepstral coefficients for audio classification purpose}},
year = {2017}
}
@article{Bharti2015a,
author = {Bharti, Roma},
journal = {International Journal of Computer Applications},
keywords = {mel frequency cepstrum coefficient,mfcc,speaker},
number = {1},
pages = {25--31},
title = {{Real Time Speaker Recognition System using MFCC and Vector Quantization Technique}},
volume = {117},
year = {2015}
}
@article{Prahallada,
author = {Prahallad, Kishore},
journal = {Technology},
pages = {1--50},
title = {{Speech Technology : A Practical Introduction Topic : Spectrogram , Cepstrum and Mel-Frequency Analysis Mel-Frequency Analysis Mel-Frequency Cepstral Coefficients}}
}
@article{Carbonneau2016b,
abstract = {Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research.},
archivePrefix = {arXiv},
arxivId = {1612.03365},
author = {Carbonneau, Marc-Andr{\'{e}} and Cheplygina, Veronika and Granger, Eric and Gagnon, Ghyslain},
doi = {10.1016/j.patcog.2017.10.009},
eprint = {1612.03365},
issn = {00313203},
pages = {1--37},
title = {{Multiple Instance Learning: A Survey of Problem Characteristics and Applications}},
url = {http://arxiv.org/abs/1612.03365},
year = {2016}
}
@article{Jiaoa,
abstract = {—The Multiple Instance Hybrid Estimator for dis-criminative target characterization from imprecisely labeled hyperspectral data is presented. In many hyperspectral target detection problems, acquiring accurately labeled training data is difficult. Furthermore, each pixel containing target is likely to be a mixture of both target and non-target signatures (i.e., sub-pixel targets), making extracting a pure prototype signature for the target class from the data extremely difficult. The proposed approach addresses these problems by introducing a data mixing model and optimizing the response of the hybrid sub-pixel detector within a multiple instance learning framework. The proposed approach iterates between estimating a set of discrim-inative target and non-target signatures and solving a sparse unmixing problem. After learning target signatures, a signature based detector can then be applied on test data. Both simulated and real hyperspectral target detection experiments show the proposed algorithm is effective at learning discriminative target signatures and achieves superior performance over state-of-the-art comparison algorithms.},
archivePrefix = {arXiv},
arxivId = {1710.11599},
author = {Jiao, Changzhe and Zare, Alina and Mcgarvey, Ronald},
eprint = {1710.11599},
keywords = {Index Terms—target detection,endmember ex-traction,hybrid detector,hyperspectral,multiple instance learning,target char-acterization},
pages = {1--14},
title = {{Multiple Instance Hybrid Estimator for Hyperspectral Target Characterization and Sub-pixel Target Detection}},
url = {https://arxiv.org/pdf/1710.11599.pdf}
}
@article{Kumar,
author = {Kumar, Anurag and Raj, Bhiksha},
title = {{WEAKLY SUPERVISED SCALABLE AUDIO CONTENT ANALYSIS}}
}
@article{Richardson2015a,
abstract = {The impressive gains in performance obtained using deep neural networks (DNNs) for automatic speech recognition (ASR) have motivated the application of DNNs to other speech technologies such as speaker recognition (SR) and language recognition (LR). Prior work has shown performance gains for separate SR and LR tasks using DNNs for direct classification or for feature extraction. In this work we present the application of single DNN for both SR and LR using the 2013 Domain Adaptation Challenge speaker recognition (DAC13) and the NIST 2011 language recognition evaluation (LRE11) benchmarks. Using a single DNN trained for ASR on Switchboard data we demonstrate large gains on performance in both benchmarks: a 55{\%} reduction in EER for the DAC13 out-of-domain condition and a 48{\%} reduction in Cavg on the LRE11 30 s test condition. It is also shown that further gains are possible using score or feature fusion leading to the possibility of a single i-vector extractor producing state-of-the-art SR and LR performance.},
archivePrefix = {arXiv},
arxivId = {1504.00923},
author = {Richardson, Fred and Member, Senior and Reynolds, Douglas and Dehak, Najim},
doi = {10.1109/LSP.2015.2420092},
eprint = {1504.00923},
isbn = {1070-9908 VO - 22},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {ASR,Bottleneck features,DAC13,DNN,Feature extraction,LR,LRE11,Mel frequency cepstral coefficient,Neural networks,Speaker recognition,Speech,Speech recognition,Training,automatic speech recognition,deep neural network approaches,direct classification,domain adaptation challenge speaker recognition,feature extraction,i-vector,language recognition,language recognition evaluation,neural nets,senone posteriors,speaker recognition,speech technologies,switchboard data,tandem features},
number = {10},
pages = {1671--1675},
title = {{Deep Neural Network Approaches to Speaker and Language Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7080838},
volume = {22},
year = {2015}
}
@article{Carbonneau2016c,
author = {Carbonneau, Marc-andr{\'{e}}},
title = {{Introduction to Multiple Instance Learning}},
year = {2016}
}
@article{Fredes2017a,
author = {Fredes, Josue and Novoa, Jose and King, Simon and Stern, Richard M and Yoma, Nestor Becerra},
doi = {10.1109/LSP.2017.2661699},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
number = {c},
pages = {1},
title = {{Locally-Normalized Filter Banks Applied to Deep Neural Network-based Robust Speech Recognition}},
url = {http://ieeexplore.ieee.org/document/7837663/},
volume = {9908},
year = {2017}
}
@article{Chauhan2017a,
abstract = {Speaker recognition is a biometrie technique which uses individual voice samples for recognition purpose. Speaker recognition is mainly divided into speaker identification and speaker verification. In this paper, a comparative study is made between various combinations of features for speaker identification. Mel frequency Cepstral Coefficient (MFCC) features are combined with spectral centroid and spectral subtraction and tested for improvement in efficiency. Feed forward artificial neural network is used as a classifier. System was tested for 30 speakers. For speaker identification, an average identification rate of 65.3{\%} is achieved when MFCC is combined with centroid features and an identification rate of 60{\%} is achieved when MFCC is combined with spectral subtraction. For speaker verification, an average verification rate of 65.7{\%} is achieved when MFCC is combined with spectral subtraction and a verification rate of 75.3{\%} is achieved when MFCC is used along with centroid.},
author = {Chauhan, Neha and Chandra, Mahesh},
doi = {10.1109/WiSPNET.2017.8299943},
isbn = {978-1-5090-4442-9},
journal = {2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)},
pages = {1147--1149},
title = {{Speaker recognition and verification using artificial neural network}},
url = {http://ieeexplore.ieee.org/document/8299943/},
year = {2017}
}
@article{Andrews2011a,
author = {Andrews, S and Tsochantaridis, I and Hofmann, T},
journal = {Advances in Neural Information Processing Systems},
title = {{Support vector machines for multiple-instance learning}},
url = {papers3://publication/uuid/22F7E194-47B2-4447-88B7-908FE72869C1},
year = {2011}
}
@article{Guo2003a,
abstract = {Support vector machines (SVMs) have been recently proposed as a new learning algorithm for pattern recognition. In this paper, the SVMs with a binary tree recognition strategy are used to tackle the audio classification problem. We illustrate the potential of SVMs on a common audio database, which consists of 409 sounds of 16 classes. We compare the SVMs based classification with other popular approaches. For audio retrieval, we propose a new metric, called distance-from-boundary (DFB). When a query audio is given, the system first finds a boundary inside which the query pattern is located. Then, all the audio patterns in the database are sorted by their distances to this boundary. All boundaries are learned by the SVMs and stored together with the audio database. Experimental comparisons for audio retrieval are presented to show the superiority of this novel metric to other similarity measures.},
author = {Guo, Guodong and Li, S Z},
doi = {10.1109/TNN.2002.806626},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Network},
number = {1},
pages = {209--215},
pmid = {18238003},
title = {{Content-based audio classification and retrieval by support vector machines.}},
volume = {14},
year = {2003}
}
@article{Lin2005a,
abstract = {In this paper, an improved audio classification and categorization technique is presented. This technique makes use of wavelets and support vector machines (SVMs) to accurately classify and categorize audio data. When a query audio is given, wavelets are first applied to extract acoustical features such as subband power and pitch information. Then, the proposed method uses a bottom-up SVM over these acoustical features and additional parameters, such as frequency cepstral coefficients, to accomplish audio classification and categorization. A public audio database (Muscle Fish), which consists of 410 sounds in 16 classes, is used to evaluate the performances of the proposed method against other similar schemes. Experimental results show that the classification errors are reduced from 16 (8.1{\%}) to six (3.0{\%}), and the categorization accuracy of a given audio sound can achieve 100{\%} in the Top 2 matches.},
author = {Lin, C.-C. and Chen, S.-H. and Truong, T.-K. and Chang, Y},
doi = {10.1109/TSA.2005.851880},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Audio categorization,audio classification,support vector machine (SVM),wavelets},
number = {5},
pages = {644--651},
title = {{Audio Classification and Categorization Based on Wavelets and Support Vector Machine}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1495445},
volume = {13},
year = {2005}
}
@article{Rong2016a,
author = {Rong, Feng},
doi = {10.1109/ICITBS.2016.98},
isbn = {978-1-5090-6061-0},
journal = {2016 International Conference on Intelligent Transportation, Big Data {\&} Smart City (ICITBS)},
pages = {81--84},
title = {{Audio Classification Method Based on Machine Learning}},
url = {http://ieeexplore.ieee.org/document/8047110/},
year = {2016}
}
@article{Fallis2013a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Support Vector Machines for Multiple-Instance Learning}},
volume = {53},
year = {2013}
}
@article{Fagerlund2007a,
abstract = {Automatic identification of bird species by their vocalization is studied in this paper. Bird sounds are represented with two different parametric representations: (i) the mel-cepstrum parameters and (ii) a set of low-level signal parameters, both of which have been found useful for bird species recognition. Recognition is performed in a decision tree with support vector machine (SVM) classifiers at each node that perform classification between two species. Recognition is tested with two sets of bird species whose recognition has been previously tested with alternative methods. Recognition results with the proposed method suggest better or equal performance when compared to existing reference methods.},
author = {Fagerlund, Seppo},
doi = {10.1155/2007/38637},
isbn = {1687-6172},
issn = {16876172},
journal = {Eurasip Journal on Advances in Signal Processing},
title = {{Bird species recognition using support vector machines}},
volume = {2007},
year = {2007}
}
