\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage[round]{natbib}
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lettrine}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{subfig}




\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\graphicspath{{"C:/Users/Conma/Documents/2019_08_Principe_Deep_Learning/Homework/Project01/Report/Images/"} {"/media/edrive/2019_08_Principe_Deep_Learning/Homework/Project01/Report/Images/"}}

\begin{document}

\title{Project 1: Manifold Learning for Fashion-MNIST Classification}
\author{\IEEEauthorblockN{Connor McCurley}
\IEEEauthorblockA{\textit{Deep Learning, Fall 2019} \\
\textit{University of Florida}\\
Gainesville, FL, USA 32611 \\
Email: cmccurley@ufl.edu}

}

\maketitle


\begin{abstract}
	
\end{abstract} 

\begin{IEEEkeywords}
Neural Network, Dimensionality Reduction, Manifold Learning, Multi-Layer Perceptron, Fashion MNIST
\end{IEEEkeywords}


%===============================================================================================================================================================================================
\section{Introduction} 

\lettrine{A}{utonomous} image classification is a challenging problem which offers potential for significant advancement in the areas of biometrics, biology, medical diagnosis, security, and more \textbf{CITE}.  This paper focuses on the use of dimensionality reduction/ manifold learning in conjunction with multi-layer perceptron artificial neural networks to automatically classify clothing items from the well-known Fashion MNIST dataset \textbf{CITE}.  \\
\indent A wide variety of approaches have been taken in attempt to solve detection and classification problems in imagery.

%===============================================================================================================================================================================================

\section{Methodology} \label{Methodology}

	\subsection{Data Analysis}
	The data was first plotted as shown in Figure \textbf{REF} to gain an understanding of the format.  Each sample in the Fashion-MNIST dataset is a 28x28, gray-scale image of a clothing item belonging to one of ten classes \cite{Xiao2017FashionMNIST}. This translates to 784 length feature vectors with values ranging between 0-255. There were exactly 60000 training images included in the  training dataset and 10000 which were held-out for test.  The 60000 samples were later sub-divided in the experimentation for cross-validation. Histograms of one-versus-all Euclidean distances to each of the classes are shown in Figure \textbf{REF} to gain a sense of class separability using the raw images, solely.  Given that the classifier would be a multi-layer perceptron artificial neural network, it was determined that dimensionality reduction should be utilized to combat the Curse of Dimensionality, while potentially improving class discriminability.
	
	\subsection{Dimensionality Reduction}
	Manifold learning, feature extraction, dimensionality reduction (DR) and representation learning are all synonymous for methods that learn representations of data that make it easier to extract useful information when building classifiers or other predictors \cite{Bengio2014RepLearningReview}. Traditionally, DR transforms high-dimensional data into meaningful representations of reduced dimensionality.  There is an expansive taxonomy of DR techniques, ranging from linear to non-linear, globally preserving to locally preserving, variance retaining to discriminability enforcing, among others \cite{VanDerMaaten2009DRReview}.  A small taxonomy of DR techniques is shown in Figure \textbf{REF}.   Dimensionality reduction has been used in a wide variety  of applications, including: speech recognition and signal processing, object recognition, computer vision, multi-task learning and domain adaptation \cite{Bengio2014RepLearningReview}, multi-modal sensor alignment \cite{zhang2010multisourceremotingsensingfusion,Davenport2010JointManifoldsDataFusion}, pose estimation \cite{Navaratnam2007JointManifoldSemiSupRegression}, land-use classification \cite{Hong2019LearnableManifoldAlignment}, medical diagnosis, meteorology, environmental monitoring, economic forecasting and more \cite{Zitova2003SurveyImageRegistrationMethods}.  The nine DR/manifold learning techniques utilized in this work are classified as either linear or non-linear and are briefly described in the following.
	
	\subsection{Linear Manifold Learning}
	The following dimensionality reduction/ manifold learning techniques solely apply linear transformations to data.  This typically means that highly nonlinear manifolds cannot be approximated well.  However, linear methods are applicable to out-of-sample datapoints, meaning the transformation can be applied to new data during test.
	\subsubsection*{Principal Component Analysis (PCA)}  PCA is arguably the most widely known, and commonly used, dimensionality reduction technique.  The goal of PCA is to transform data along its principle axes so that maximum variance is retained in the new coordinate space \cite{Tipping1999PPCA,Murphy2012Textbook}.  A key assumption when using PCA as a preprocessing step for classification is that the most discriminative features are also the most highly varying.
	\subsubsection*{Fisher's Linear Discriminant Analysis (FLDA)}  Compared to PCA, whose ultimate goal is to maximize the variance in each orthogonal dimension of the latent space, the objective of FLDA is to maximize discriminability through a supervised linear projection of data.  This is realized by solving a generalized eigenvalue problem, thus minimizing intra-class variability and maximizing inter-class variability \cite{Murphy2012Textbook,Sugiyama2006FDASupDimRed}.
	
	
	\subsection{Nonlinear Manifold Learning}
	\subsubsection*{t-Distributed Stochastic Neighbor Embedding (t-SNE)} t-SNE is currently a state-of-the-art method for dimensionality reduction and data visualization. Stochastic Neighbor Embedding (SNE) uses radial basis functions  to convert high-dimensional Euclidean distances between datapoints into conditional probabilities representing similarities.  SNE then uses a cost based on KL divergence to match pairwise distributions between data representations in the high and low-dimensional spaces.  Since KL divergence is asymmetric, different types of errors are not weighted equally in the cost.  For example, there is a larger penalty for using very dissimilar points to represent nearby datapoints.  t-SNE is a modification of SNE in which the RBF kernels are replaced by the heavy-tailed student t distribution \cite{vanDerMaaten2008tSNE}.  By placing emphasis on the pairwise distances of datapoints, t-SNE captures both local manifold information and global structure such as the presence of clusters at several scales.
	\subsubsection*{Uniform Manifold Approximation and Projection (UMAP)}
	\subsubsection*{Auto-encoder} An auto-encoder is a specific taxonomy of artificial neural networks whose output is the same dimensionality as the input and whose desired is actually the input sample \cite{Haykin2009NeuralNetworks,Goodfellow2016DeepLearning}. Typically, auto-encoders enforce dimensionality reduction operations up to their middle layers.  This portion of the network is known as the `encoder', since it projects data into a lower dimensional space.  The second half of the network projects the data back into its original dimensionality in attempt to reconstruct the original sample.  This section of the network is known as the `decoder'. (See Figure \textbf{REF}.)  In practice, samples can be passed through the encoder to perform dimensionality reduction.
	\subsubsection*{Self-Organizing Feature Map (SOM)} Kohonen's self-organizing feature map is a form of artificial neural network that operates with Competitive Hebbian Learning \cite{Haykin2009NeuralNetworks,Kohonen1990SOM,Fritzke1995GrowingNeuralGas}.  The network is initialized in a 2D or 3D lattice, whose coordinates represent structural relationships between the nodes (or units).  Each sample is introduced to the network and all nodes in a neighborhood of the `best-matching unit' are pulled closer to the sample. While SOMs are, traditionally, unsupervised learning methods, label information can be incorporated to inform clustering.  The supervised version of a SOM  is known as Learning Vector Quantization (LVQ).
	\subsubsection*{Isometric Feature Mapping (ISOMAP)} ISOMAP is dimensionality reduction method which attempts to retain global geodesic distance of data \cite{Tenenbaum2000Isomap}.  A graph is constructed which represents pairwise distances between all data samples before  applying classical multi-dimensional scaling to perform dimensionality reduction.  It has been shown that ISOMAP effectively estimates geodesic distance using Euclidean distance, however, it suffers from large, dense Gram matrices which can render solutions intractable \cite{Thorstensen2009ManifoldThesis}.  Moreover, ISOMAP fails for manifolds which are non-convex or contain holes \cite{VanDerMaaten2009DRReview}.
	\subsubsection*{Locally Linear Embedding (LLE)} Locally Linear Embedding aims at retaining local pairwise distances through dimensionality reduction \cite{Roweis2000LLE,Saul2001LLEIntro}.  In contrast to ISOMAP, LLE attempts to preserve local structure.  This is done by first constructing a local neighborhood graph then optimizing an objective which reconstructs data points in the reduced dimensional space as weighted linear combinations of its neighbors.  the local embedding exhibited by LLE often allows for successful embeddings of nonlinear manifolds.
	\subsubsection*{Laplacian Eigenmaps}  The method of Laplacian Eigenmaps is similar to LLE by the fact that it preserves local manifold information.  Laplacian Eigenmaps use spectral graph theory to minimize the distances of neighboring points in a low-dimensional space \cite{Belkin2003LaplacianEigenmaps,VanDerMaaten2009DRReview}.  Supervised and semi-supervised versions of Laplacian Eigenmaps have been developed which enforce inta-class similarity and inter-class dissimilarity.  
	 

	\subsection{Network Architecture}
	
	
	
%	\begin{center}
%		\begin{figure*}[h]
%			\centering
%			\includegraphics[width=\textwidth]{"Raw"}
%			\caption{Raw audio signals. Top: Training segments of 10 unique manatee categories.  Middle: Raw training data for background noise.  Bottom: Raw test signal colored by hand-created labels.  Red portions signify segments from 16 unique manatee observations, while blue represents background noise.}
%			\label{fig:Raw}
%		\end{figure*}
%	\end{center}
	





	


\subsection{Experiments} \label{Experiments}



%===============================================================================================================================================================================================
\section{Results} \label{Results}





%\begin{table}[h!]
%	\caption{AUC for Various Signature Generation Methods}
%	\label{tab:AUC}
%	\normalsize
%	\begin{center}
%		\begin{tabularx}{0.5\textwidth}{ |X|X| } 
%			\hline
%			\textbf{Signature Creation}  & \textbf{AUC} \\
%			\hline
%			Average & 0.8406 \\
%			\hline
%			\textbf{Median} & \textbf{0.8414} \\
%			\hline
%			Combined Manatee Bag & 0.4472 \\
%			\hline
%			Individual Manatee Bags & 0.7379 \\
%			\hline
%			Six Bags per Manatee & 0.8133 \\
%			\hline
%			10 Bags per Manatee & 0.8085 \\
%			\hline
%		\end{tabularx}
%	\end{center}
%\end{table} 


%===============================================================================================================================================================================================

\section{Discussion} \label{Discussion}
In this sections, observations are made on results and insight is given to potential influences.

\subsection{Results}

\subsection{Effects of Manifold Learning/ Dimensionality Reduction}


\subsection{Potential Improvements}



%===============================================================================================================================================================================================
\section{Conclusions} \label{Conclusions}
A
%===============================================================================================================================================================================================

\section*{Honor Statement}
\noindent
* I confirm that this assignment is my own work, it is not copied from any other person's work (published or unpublished), and has not been previously submitted for assessment either at University of Florida or elsewhere.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.20\textwidth]{"Signature"}
\end{figure}


\newpage

\bibliography{Project1}
\bibliographystyle{IEEEtran}
%\bibliographystyle{plainnat}

\end{document}
