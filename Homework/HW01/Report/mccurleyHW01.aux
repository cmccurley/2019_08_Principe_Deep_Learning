\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture implemented to solve the mask problem. The network implements a two hidden layer MLP with two input units, 13 units in the first hidden layer, 4 hidden units in the second layer, and 1 output neuron. Sign functions were used for activations in the network. \relax }}{1}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:architectureQ1}{{1}{1}{Architecture implemented to solve the mask problem. The network implements a two hidden layer MLP with two input units, 13 units in the first hidden layer, 4 hidden units in the second layer, and 1 output neuron. Sign functions were used for activations in the network. \relax }{figure.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Weights in the First Hidden Layer\relax }}{2}{table.caption.3}}
\newlabel{tab:WeightsFirstHiddenLayer}{{1}{2}{Weights in the First Hidden Layer\relax }{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Outputs of the second hidden layer in the ``mask network''. It can be observed that each unit in this layer determines whether a sample falls within a desired mask region.\relax }}{2}{figure.caption.4}}
\newlabel{fig:maskComponents}{{2}{2}{Outputs of the second hidden layer in the ``mask network''. It can be observed that each unit in this layer determines whether a sample falls within a desired mask region.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Nose}}}{2}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Mouth}}}{2}{subfigure.2.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Left Eye}}}{2}{subfigure.2.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Right Eye}}}{2}{subfigure.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Weights in the Second Hidden Layer\relax }}{3}{table.caption.5}}
\newlabel{tab:WeightsSecondHiddenLayer}{{2}{3}{Weights in the Second Hidden Layer\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training data points for the Star Problem. Class labels are denoted by color. \relax }}{3}{figure.caption.7}}
\newlabel{fig:Q2Data}{{3}{3}{Training data points for the Star Problem. Class labels are denoted by color. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architecture implemented to solve the Star Problem. The network consists of a single hidden layer MLP with two input units, 13 units in the first hidden layer, 4 hidden units in the second layer, and 1 output neuron. Sigmoid activation functions were used at each neuron in the network. \relax }}{4}{figure.caption.8}}
\newlabel{fig:architectureQ2}{{4}{4}{Architecture implemented to solve the Star Problem. The network consists of a single hidden layer MLP with two input units, 13 units in the first hidden layer, 4 hidden units in the second layer, and 1 output neuron. Sigmoid activation functions were used at each neuron in the network. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Decision regions learned by the single hidden layer network shown in Figure \ref  {fig:architectureQ2}. The regions partially solve the Star Problem, but fail for out-of-sample data points. \relax }}{4}{figure.caption.9}}
\newlabel{fig:Q2RegionsInexact}{{5}{4}{Decision regions learned by the single hidden layer network shown in Figure \ref {fig:architectureQ2}. The regions partially solve the Star Problem, but fail for out-of-sample data points. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning curve for the network shown in Figure \ref  {fig:architectureQ2}. This shows that the network's training error falls below 0.002 after approximately 1000 iterations of training with a learning rate of 0.8.\relax }}{5}{figure.caption.10}}
\newlabel{fig:Q2RegionsInexact}{{6}{5}{Learning curve for the network shown in Figure \ref {fig:architectureQ2}. This shows that the network's training error falls below 0.002 after approximately 1000 iterations of training with a learning rate of 0.8.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Weight tracks for the network shown in Figure \ref  {fig:architectureQ2}. This shows that the network's weights converge after approximately 1000 iterations of training.\relax }}{5}{figure.caption.11}}
\newlabel{fig:Q2RegionsInexact}{{7}{5}{Weight tracks for the network shown in Figure \ref {fig:architectureQ2}. This shows that the network's weights converge after approximately 1000 iterations of training.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Decision regions learned by the single hidden layer network shown in Figure \ref  {fig:architectureQ2}, with constraints to learn only horizontal and vertical decision boundaries. The regions perfectly solve the Star Problem. \relax }}{6}{figure.caption.12}}
\newlabel{fig:Q2RegionsInexact}{{8}{6}{Decision regions learned by the single hidden layer network shown in Figure \ref {fig:architectureQ2}, with constraints to learn only horizontal and vertical decision boundaries. The regions perfectly solve the Star Problem. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Learning curve for the network shown in Figure \ref  {fig:architectureQ2} with constraints to learn only horizontal and vertical decision boundaries. This shows that the network's training error falls below 0.002 after approximately 1200 iterations of training with a learning rate of 1.2.\relax }}{6}{figure.caption.13}}
\newlabel{fig:Q2RegionsInexact}{{9}{6}{Learning curve for the network shown in Figure \ref {fig:architectureQ2} with constraints to learn only horizontal and vertical decision boundaries. This shows that the network's training error falls below 0.002 after approximately 1200 iterations of training with a learning rate of 1.2.\relax }{figure.caption.13}{}}
\bibcite{Principe}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Weight tracks for the network shown in Figure \ref  {fig:architectureQ2} with constraints to learn only horizontal and vertical decision boundaries. This shows that the network's weights converge after approximately 1200 iterations of training.\relax }}{7}{figure.caption.14}}
\newlabel{fig:Q2RegionsInexact}{{10}{7}{Weight tracks for the network shown in Figure \ref {fig:architectureQ2} with constraints to learn only horizontal and vertical decision boundaries. This shows that the network's weights converge after approximately 1200 iterations of training.\relax }{figure.caption.14}{}}
