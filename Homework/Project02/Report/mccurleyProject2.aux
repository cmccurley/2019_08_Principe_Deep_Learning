\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Prasad2015Review,Lu2007Review}
\citation{Xiao2017FashionMNIST}
\citation{Mensink2013KNN}
\citation{Sanchez2011svm,Lin2011svm}
\citation{Shao2019DictionaryLearning}
\citation{Timofte2013NaiveBayes}
\citation{Swaroop2016TemplateMatching}
\citation{Driss2017MLPandCNN}
\citation{Prasad2015Review,Lu2007Review}
\citation{Principe2010ITL}
\citation{McCurley2019PrincipeProject1}
\citation{Xiao2017FashionMNIST}
\citation{Haykin2009NeuralNetworks,Goodfellow2016DeepLearning}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methodology}{1}{section.2}}
\newlabel{Methodology}{{II}{1}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Data Analysis}{1}{subsection.2.1}}
\citation{Lin2011svm,Sanchez2011svm}
\citation{Murphy2012Textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization. The gray-scale images are size 28x28, each representing an article of clothing.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:samples}{{1}{2}{Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization. The gray-scale images are size 28x28, each representing an article of clothing.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Autoencoder}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Support Vector Machines}{2}{subsection.2.3}}
\citation{Goodfellow2016DeepLearning,Haykin2009NeuralNetworks}
\citation{Xiao2017FashionMNIST}
\citation{Principe2010ITL}
\citation{Principe2010ITL}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Block diagram of an autoencoder neural network. The layers consecutively reduce dimensionality until the middle (bottleneck) layer. The second half of the network transforms the data back to the size of the input. The desired value of the network is the original image.\relax }}{3}{figure.caption.8}}
\newlabel{fig:autoencoder}{{2}{3}{Block diagram of an autoencoder neural network. The layers consecutively reduce dimensionality until the middle (bottleneck) layer. The second half of the network transforms the data back to the size of the input. The desired value of the network is the original image.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Baseline CNN}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-E}}Information Theoretic Learning}{3}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{3}{section.3}}
\newlabel{Results}{{III}{3}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Autoencoder Reconstruction}{3}{subsection.3.1}}
\newlabel{ae_reconstruction}{{\unhbox \voidb@x \hbox {III-A}}{3}{Autoencoder Reconstruction}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Confusion Matrices}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality 10 (a) and 100 (b). The images' original dimensionality was 784.\relax }}{4}{figure.caption.15}}
\newlabel{fig:ae_reconstruction}{{3}{4}{Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality 10 (a) and 100 (b). The images' original dimensionality was 784.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}{subfigure.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Comparison of Bottleneck Size}{4}{subsection.3.3}}
\newlabel{classification_accuracy}{{\unhbox \voidb@x \hbox {III-C}}{4}{Comparison of Bottleneck Size}{subsection.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Classification Accuracies for Different Neural Model/ Classification Systems\relax }}{4}{table.caption.16}}
\newlabel{tab:step1comparison}{{I}{4}{Classification Accuracies for Different Neural Model/ Classification Systems\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Comparison of xEnt Kernel Bandwidths}{4}{subsection.3.4}}
\newlabel{bw_comparison}{{\unhbox \voidb@x \hbox {III-D}}{4}{Comparison of xEnt Kernel Bandwidths}{subsection.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Classification Accuracies for Varying xEnt Kernel Bandwidths and Latent Dimensionalities\relax }}{4}{table.caption.18}}
\newlabel{tab:bw_comparison}{{II}{4}{Classification Accuracies for Varying xEnt Kernel Bandwidths and Latent Dimensionalities\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Discussion}{4}{section.4}}
\newlabel{Discussion}{{IV}{4}{Discussion}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Confusion matrix for the baseline CNN (a), SVM trained on data passed through an encoder network with bottleneck dimensionality 100 (b) and SVM trained on data passed through an encoder network with bottleneck dimensionality 10 (c). The CNN model classified 90\% of the test data correctly while the SVM 100D model classified 86\% correctly and the SVM 10D classified 76\% correctly.\relax }}{5}{figure.caption.17}}
\newlabel{fig:step_1_conf_mat}{{4}{5}{Confusion matrix for the baseline CNN (a), SVM trained on data passed through an encoder network with bottleneck dimensionality 100 (b) and SVM trained on data passed through an encoder network with bottleneck dimensionality 10 (c). The CNN model classified 90\% of the test data correctly while the SVM 100D model classified 86\% correctly and the SVM 10D classified 76\% correctly.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.4.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{5}{subfigure.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrix for the MLP with a final dimensionality (before output) of $k=100$ and a xEnt bandwidth of $\sigma =300$. The model classified 89\% of the test data correctly.\relax }}{5}{figure.caption.19}}
\newlabel{fig:feature_size_100_bw_300}{{5}{5}{Confusion matrix for the MLP with a final dimensionality (before output) of $k=100$ and a xEnt bandwidth of $\sigma =300$. The model classified 89\% of the test data correctly.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Results}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Potential Improvements}{5}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Confusion matrix for the MLP with a final dimensionality (before output) of $k=75$ and a xEnt bandwidth of $\sigma =300$. The model classified 74\% of the test data correctly.\relax }}{5}{figure.caption.20}}
\newlabel{fig:feature_size_75_bw_300}{{6}{5}{Confusion matrix for the MLP with a final dimensionality (before output) of $k=75$ and a xEnt bandwidth of $\sigma =300$. The model classified 74\% of the test data correctly.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions}{5}{section.5}}
\newlabel{Conclusions}{{V}{5}{Conclusions}{section.5}{}}
\bibdata{Project2}
\bibcite{Prasad2015Review}{1}
\bibcite{Lu2007Review}{2}
\bibcite{Xiao2017FashionMNIST}{3}
\bibcite{Mensink2013KNN}{4}
\bibcite{Sanchez2011svm}{5}
\bibcite{Lin2011svm}{6}
\bibcite{Shao2019DictionaryLearning}{7}
\bibcite{Timofte2013NaiveBayes}{8}
\bibcite{Swaroop2016TemplateMatching}{9}
\bibcite{Driss2017MLPandCNN}{10}
\bibcite{McCurley2019PrincipeProject1}{11}
\bibcite{Haykin2009NeuralNetworks}{12}
\bibcite{Goodfellow2016DeepLearning}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Confusion matrix for the MLP with a final dimensionality (before output) of $k=50$ and a xEnt bandwidth of $\sigma =3$. The model classified 88\% of the test data correctly.\relax }}{6}{figure.caption.21}}
\newlabel{fig:feature_size_50_bw_3}{{7}{6}{Confusion matrix for the MLP with a final dimensionality (before output) of $k=50$ and a xEnt bandwidth of $\sigma =3$. The model classified 88\% of the test data correctly.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confusion matrix for the MLP with a final dimensionality (before output) of $k=25$ and a xEnt bandwidth of $\sigma =300$. The model classified 73\% of the test data correctly.\relax }}{6}{figure.caption.22}}
\newlabel{fig:feature_size_25_bw_300}{{8}{6}{Confusion matrix for the MLP with a final dimensionality (before output) of $k=25$ and a xEnt bandwidth of $\sigma =300$. The model classified 73\% of the test data correctly.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Confusion matrix for the MLP with a final dimensionality (before output) of $k=10$ and a xEnt bandwidth of $\sigma =300$. The model classified 80\% of the test data correctly.\relax }}{6}{figure.caption.23}}
\newlabel{fig:feature_size_10_bw_300}{{9}{6}{Confusion matrix for the MLP with a final dimensionality (before output) of $k=10$ and a xEnt bandwidth of $\sigma =300$. The model classified 80\% of the test data correctly.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.26}}
\bibcite{Murphy2012Textbook}{14}
\bibcite{Principe2010ITL}{15}
\bibstyle{IEEEtran}
