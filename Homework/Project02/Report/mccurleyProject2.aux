\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Prasad2015Review,Lu2007Review}
\citation{Xiao2017FashionMNIST}
\citation{Mensink2013KNN}
\citation{Sanchez2011svm,Lin2011svm}
\citation{Shao2019DictionaryLearning}
\citation{Timofte2013NaiveBayes}
\citation{Swaroop2016TemplateMatching}
\citation{Driss2017MLPandCNN}
\citation{Prasad2015Review,Lu2007Review}
\citation{Principe2010ITL}
\citation{McCurley2019PrincipeProject1}
\citation{Xiao2017FashionMNIST}
\citation{Haykin2009NeuralNetworks,Goodfellow2016DeepLearning}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methodology}{1}{section.2}}
\newlabel{Methodology}{{II}{1}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Data Analysis}{1}{subsection.2.1}}
\citation{Lin2011svm,Sanchez2011svm}
\citation{Murphy2012Textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization. The gray-scale images are size 28x28, each representing an article of clothing.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:samples}{{1}{2}{Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization. The gray-scale images are size 28x28, each representing an article of clothing.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Autoencoder}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Support Vector Machines}{2}{subsection.2.3}}
\citation{Goodfellow2016DeepLearning,Haykin2009NeuralNetworks}
\citation{Xiao2017FashionMNIST}
\citation{Principe2010ITL}
\citation{Principe2010ITL}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Block diagram of an autoencoder neural network. The layers consecutively reduce dimensionality until the middle (bottleneck) layer. The second half of the network transforms the data back to the size of the input. The desired value of the network is the original image.\relax }}{3}{figure.caption.8}}
\newlabel{fig:autoencoder}{{2}{3}{Block diagram of an autoencoder neural network. The layers consecutively reduce dimensionality until the middle (bottleneck) layer. The second half of the network transforms the data back to the size of the input. The desired value of the network is the original image.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Baseline CNN}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-E}}Information Theoretic Learning}{3}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{3}{section.3}}
\newlabel{Results}{{III}{3}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Autoencoder Reconstruction}{3}{subsection.3.1}}
\newlabel{ae_reconstruction}{{\unhbox \voidb@x \hbox {III-A}}{3}{Autoencoder Reconstruction}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Confusion Matrices}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality (a) 10 and (b) 100. The images' original dimensionality was 784.\relax }}{4}{figure.caption.15}}
\newlabel{fig:ae_reconstruction}{{3}{4}{Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality (a) 10 and (b) 100. The images' original dimensionality was 784.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}{subfigure.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Comparison of Bottleneck Size}{4}{subsection.3.3}}
\newlabel{classification_accuracy}{{\unhbox \voidb@x \hbox {III-C}}{4}{Comparison of Bottleneck Size}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Comparison of xEnt Kernel Bandwidths}{4}{subsection.3.4}}
\newlabel{bw_comparison}{{\unhbox \voidb@x \hbox {III-D}}{4}{Comparison of xEnt Kernel Bandwidths}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Discussion}{4}{section.4}}
\newlabel{Discussion}{{IV}{4}{Discussion}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Classification Accuracies for the Baseline CNN and AE + SVMs with Varying Bottleneck Dimensionalities\relax }}{4}{table.caption.17}}
\newlabel{tab:step1comparison}{{I}{4}{Classification Accuracies for the Baseline CNN and AE + SVMs with Varying Bottleneck Dimensionalities\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Classification Accuracies for Varying xEnt Kernel Bandwidths and Latent Dimensionalities\relax }}{4}{table.caption.18}}
\newlabel{tab:bw_comparison}{{II}{4}{Classification Accuracies for Varying xEnt Kernel Bandwidths and Latent Dimensionalities\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Results and Experimental Design}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Confusion matrix for the (a) baseline CNN, (b) SVM trained on data passed through an encoder network with bottleneck dimensionality 100 and (c) SVM trained on data passed through an encoder network with bottleneck dimensionality 10. The CNN model classified 90\% of the test data correctly while the SVM 100D model classified 86\% correctly and the SVM 10D classified 76\% correctly.\relax }}{5}{figure.caption.16}}
\newlabel{fig:step_1_conf_mat}{{4}{5}{Confusion matrix for the (a) baseline CNN, (b) SVM trained on data passed through an encoder network with bottleneck dimensionality 100 and (c) SVM trained on data passed through an encoder network with bottleneck dimensionality 10. The CNN model classified 90\% of the test data correctly while the SVM 100D model classified 86\% correctly and the SVM 10D classified 76\% correctly.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.4.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{5}{subfigure.4.3}}
\bibdata{Project2}
\bibcite{Prasad2015Review}{1}
\bibcite{Lu2007Review}{2}
\bibcite{Xiao2017FashionMNIST}{3}
\bibcite{Mensink2013KNN}{4}
\bibcite{Sanchez2011svm}{5}
\bibcite{Lin2011svm}{6}
\bibcite{Shao2019DictionaryLearning}{7}
\bibcite{Timofte2013NaiveBayes}{8}
\bibcite{Swaroop2016TemplateMatching}{9}
\bibcite{Driss2017MLPandCNN}{10}
\bibcite{McCurley2019PrincipeProject1}{11}
\bibcite{Haykin2009NeuralNetworks}{12}
\bibcite{Goodfellow2016DeepLearning}{13}
\bibcite{Murphy2012Textbook}{14}
\bibcite{Principe2010ITL}{15}
\bibstyle{IEEEtran}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Potential Improvements}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions}{6}{section.5}}
\newlabel{Conclusions}{{V}{6}{Conclusions}{section.5}{}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrices for the ITL MLPs with a final dimensionality (before output) of (a) $k=100$ and a xEnt bandwidth of $\sigma =300$ which classified 89\% of the test data correctly, (b) $k=75$ and a xEnt bandwidth of $\sigma =300$ which classified 74\% of the test data correctly, (c) $k=50$ and a xEnt bandwidth of $\sigma =3$ which classified 88\% of the test data correctly, (d) $k=25$ and a xEnt bandwidth of $\sigma =300$ which classified 73\% of the test data correctly and (e) $k=10$ and a xEnt bandwidth of $\sigma =300$ which classified 80\% of the test data correctly.\relax }}{7}{figure.caption.21}}
\newlabel{fig:step_2_conf_mat}{{5}{7}{Confusion matrices for the ITL MLPs with a final dimensionality (before output) of (a) $k=100$ and a xEnt bandwidth of $\sigma =300$ which classified 89\% of the test data correctly, (b) $k=75$ and a xEnt bandwidth of $\sigma =300$ which classified 74\% of the test data correctly, (c) $k=50$ and a xEnt bandwidth of $\sigma =3$ which classified 88\% of the test data correctly, (d) $k=25$ and a xEnt bandwidth of $\sigma =300$ which classified 73\% of the test data correctly and (e) $k=10$ and a xEnt bandwidth of $\sigma =300$ which classified 80\% of the test data correctly.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{subfigure.5.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{subfigure.5.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{7}{subfigure.5.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{7}{subfigure.5.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{7}{subfigure.5.5}}
