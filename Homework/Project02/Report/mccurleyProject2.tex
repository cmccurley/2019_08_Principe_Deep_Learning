\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage[round]{natbib}
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lettrine}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{subfig}





\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\graphicspath{{"C:/Users/Conma/Documents/2019_08_Principe_Deep_Learning/Homework/Project02/Report/Images/"} {"/media/edrive/2019_08_Principe_Deep_Learning/Homework/Project02/Report/Images/"}}

\begin{document}

\title{Project 2: A Comparative Study Between Traditional and Information Theoretic Neural Network Models for Fashion-MNIST Classification}
\author{\IEEEauthorblockN{Connor McCurley}
\IEEEauthorblockA{\textit{Deep Learning, Fall 2019} \\
\textit{University of Florida}\\
Gainesville, FL, USA 32611 \\
Email: cmccurley@ufl.edu}

}

\maketitle


\begin{abstract}
	This paper explores the utilization of information theory in supervised classification of Fashion-MNIST images.  Autoencoder neural networks were trained and used in conjunction with support vector machines to perform classification.  Similar multilayer perceptron networks were trained using information theoretic learning.  Both models were compared to a baseline convolutional neural network to gauge differences in performance.  Classifier performance was evaluated using overall accuracy.  Additionally, confusion matrices provided insight to each classifiers' confusers.  The baseline CNN obtained a test classification accuracy of 90\%, which was the highest amongst the methods compared.  This was followed closely by the information theoretic neural network.  Due to architectural differences which should have limited performance when compared to the CNN, the accuracy achieved by the information theoretic network suggests the usefulness of ITL in the learning process.  It is believed that performance could be improved by considering further classification refinement on the difficult classes, namely: shirt, coat and pullover.
\end{abstract} 

\begin{IEEEkeywords}
Fashion MNIST, Autoencoder, Information Theoretic Learning, Support Vector Machine
\end{IEEEkeywords}

%==================================================================================================
%====================================  Introduction ===============================================
%==================================================================================================
\section{Introduction} 

\lettrine{A}{utonomous} image classification is a challenging problem which offers potential for significant advancement in the areas of biometrics, biology, medical diagnosis, security, and more \cite{Prasad2015Review,Lu2007Review}.  This paper provides a comparison of information theoretic  to non-information theoretic learning approaches for autonomous classification of  clothing items in the well-known Fashion MNIST dataset \cite{Xiao2017FashionMNIST}.  \\
\indent A wide variety of approaches have been taken in attempt to solve detection and classification problems in imagery.  \cite{Mensink2013KNN} used dissimilarity-based classifiers along with metric learning to dually drive samples toward their respective class representatives while also enforcing separation between classes.  \cite{Sanchez2011svm,Lin2011svm} utilized vector embeddings with linear support vector machines to discriminate between low-dimensional image representations.  The work in \cite{Shao2019DictionaryLearning} found sparse weighted combinations of dictionary atoms to accurately reconstruct images where specific bases equated to the various  classes.  The authors of \cite{Timofte2013NaiveBayes} utilized statistical properties to match samples to generating distributions.  The work in \cite{Swaroop2016TemplateMatching} employed traditional template matching to locate objects or compositions in imagery.  The review in \cite{Driss2017MLPandCNN} demonstrated the expansive uses of artificial neural networks in image classification.  This, of course, is just a small sample of image classification techniques.  The reviews in \cite{Prasad2015Review,Lu2007Review} elaborate extensively on the myriad of methods.  A commonality among the discussed methods is that they fail to take advantage of higher-order statistics to capture the error between prediction values \cite{Principe2010ITL}.  To this end, this work explores the utility of information theoretic cost in terms of classification performance on the Fashion-MNIST dataset.\\
\indent The remainder of this paper is organized as follows.  Section \ref{Methodology} describes the methodology used in developing three disparate classification systems utilizing artificial neural networks.  Classification results are presented in Section \ref{Results}.  Practical insights to results are given in Section \ref{Discussion}.  Finally, Section \ref{Conclusions} reveals concluding remarks and discusses future lines of research.

%==================================================================================================
%====================================  Methodology ================================================
%==================================================================================================
\vspace{-0.3cm}
\section{Methodology} \label{Methodology}
	This section describes the methodology implemented in this work.  Analysis of the data is performed, dimensionality reduction and classification procedures are described, various network architectures under analysis are elaborated on and experimental procedures are outlined.

%% Fashion MNIST data samples
\begin{center}
	\begin{figure*}[ht!]
		\centering
		\includegraphics[width=0.8\textwidth]{"samples/samples"}
		\caption{Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization.  The gray-scale images are size 28x28, each representing an article of clothing.}
		\label{fig:samples}
	\end{figure*}
\end{center}

	\vspace{-1cm}
	%% Data analysis
	\subsection{Data Analysis}
	 The following data analysis was originally described in \cite{McCurley2019PrincipeProject1}, but is pertinent to this work and is thus re-analyzed here.  The data was plotted as shown in Figure \ref{fig:samples} to gain an understanding of the format.  Each sample in the Fashion-MNIST dataset is a 28x28, gray-scale image of a clothing item belonging to one of ten classes \cite{Xiao2017FashionMNIST}. This translates to 784 length feature vectors with values ranging between 0-255. There were exactly 60000 training images included in the  training dataset and 10000 which were held-out for test.  The 60000 samples were later sub-divided in the experimentation for cross-validation.  As with Project 1, dimensionality reduction(DR) was incorporated to reduce data complexity.  This DR was employed through the use of stacked autocoder neural networks (SAE).  Elaboration on the SAE networks is provided in the following section.

	\vspace{-0.1cm}
	%% Autoencoder
	 \subsection{Autoencoder} 
	 \subsubsection*{Description}
	 An autoencoder is a specific taxonomy of artificial neural network which learns how to  compress and de-compress representations of data \cite{Haykin2009NeuralNetworks,Goodfellow2016DeepLearning}. The first half of an autoencoder typically performs dimensionality reduction through non-linear transformations until the middle layer, known as the \textit{bottleneck} or \textit{latent} layer.  The goal of the \textit{encoder} is to learn efficient representations of the factors which govern variation in the data, or in terms of compression, \textit{codes} which can be used to reconstruct the input  data with high accuracy. The second half of an SAE (which is called the \textit{decoder}) projects the data back into its original dimensionality in attempt to reconstruct the original sample.(See Figure \ref{fig:autoencoder}.) Reconstruction loss is between the input and output is used to update the network's parameters.  In practice, samples can be passed through the encoder to perform dimensionality reduction. 
	 
  	%% Architecture parameters
  	\subsubsection*{SAE Architecture}
  	The SAE architecture tested in this work consisted of 5 hidden layers, along with the input and output.  The layers were selected as $784 \rightarrow 500 \rightarrow 200 \rightarrow k \rightarrow 200 \rightarrow 500 \rightarrow 784$, where $k$ is the arbitrarily chosen dimensionality of the bottleneck.  In this work, $k$ was tested at $[10,25,50,75,100]$ in order to provided a reasonable comparison of performance changes resulting from dimensionality reduction.  ReLU activation functions were used to apply nonlinearity.  A sigmoid activation, however, was used at the output layer to enforce image value constraints between $[0-1]$.  This was done because the images were normalized between $[0-1]$ before passing through the network.  An initial learning rate of $\eta=0.01$ was selected, and was updated using the Adamax optimizer through training.
  	
  	%% Description of experiments
  	\subsubsection*{SAE Experiments}
  	The SAE network was trained for 20 epochs using mini-batch sizes of 200 samples.  The bottleneck layer's dimensionality was varied between $[10,25,50,75,100]$.  Each network configuration was trained 5 times, and the model which provided the lowest reconstruction  Mean-Squared Error (MSE) on the hold-out validation set was selected for further use in classification.  Results are shown in section \ref{ae_reconstruction}.
  	
	 %% Support vector Machines
	 \subsection{Support Vector Machines}
	 \subsubsection*{Description}
	 A Support Vector Machine (SVM) is a specific class of sparse kernel machine whose objective  is to learn a decision boundary which can adequately discriminate between classes in a high-dimensional space \cite{Lin2011svm,Sanchez2011svm}.  Because of its sparsity constraints, a SVMs' predictions rely only on a subset of the training data known as \textit{support vectors}. By design, support vectors tend to be examples in the training data which lie closest to the decision boundary, and are thus the most prone to mis-classification. A primary difference between SVM and methods relying on Information Theory is that vanilla SVM classification predictions are not probabilistic \cite{Murphy2012Textbook}.  In other words, hard labels are assigned which do not capture the uncertainty of the prediction results.  In this work, SVMs were trained on the data passed through the selected autoencoders.  A comparison of classification performance on the various sizes of features is provided in section \ref{classification_accuracy}.
	 
	 \subsubsection*{Parameters}
	 Non-linear support vector machines were used as the classifiers in this work. The necessary hyperparameters were the type of mapping kernel,  kernel parameters, and a regularization (slack) parameter.  A radial-basis function was arbitrarily chosen as the mapping function.  Silverman's rule was used to provided a reasonable range for the kernel bandwidth. The regularization parameter was set to the Python Scikitlearn's default value of $C=1$.  These parameter choices provided reasonable results for comparison.  More parameter variations for the SVM were not tested due to time limitations.
	 
	 \subsubsection*{Experiments}
	 SVMs were employed using Scikitlearn's SVM SVC package.  The classifiers were trained using one-versus-one training.  At test, a sample was applied to the classifier ensemble and the most-likely label was provided to the sample.  Data was passed through each of the top 5 trained autoencoders, and a single SVM was trained to provide label predictions for each input feature dimensionality, $[10,25,50,75,100]$.  Classification performance is presented in section \ref{classification_accuracy}. 
	 
	 %% Autoencoder network
	 \begin{center}
	 	\begin{figure}[t]
	 		\centering
	 		\includegraphics[width=0.4\textwidth]{"autoencoder"}
	 		\caption{Block diagram of an autoencoder neural network.  The layers consecutively reduce dimensionality until the middle (bottleneck) layer.  The second half of the network transforms the data back to the size of the input.  The desired value of the network is the original image.}
	 		\label{fig:autoencoder}
	 	\end{figure}
	 \end{center}
	 
	 \vspace{-1.3cm}
	 \subsection{Baseline CNN}
	 
	 \subsubsection*{Description}
	 The autoencoder + SVM classifiers were compared against a baseline convolutional neural network (CNN) architecture.  A CNN is a special type of neural network which utilizes weight sharing. A suit of kernels is convolved across the input feature map to detect interesting attributes in an image \cite{Goodfellow2016DeepLearning,Haykin2009NeuralNetworks}.  CNNs have shown considerable success in a variety of image classification problems.  For this reason, a CNN architecture is used as a standard for comparison.  Classification performance of the baseline architecture is provided in section \ref{classification_accuracy}.
	 
	 %% Architecture
	 \subsubsection*{CNN Architecture}
	 The baseline CNN architecture was chosen based off previous performance on Fashion-MNIST classification \cite{Xiao2017FashionMNIST}.  The network consists of two convolutional/ max-pooling layers followed by two fully connected layers.  The network provides an output size of 10.  This is passed into a soft-max activation function to provide class probability scores.  ReLUs are used to provide nonlinearity in each of the hidden layers. 
	 
	 %% Training
	 \subsubsection*{CNN Experiments}
	 The baseline CNN was initialized with a learning rate of $\eta=0.01$.  This was refined during training with the Adamax optimizer.  The model was trained 5 times for 20 epochs each.  Mini-batches of size 200 were implemented and class prediction scores were evaluated using cross-entropy loss.  The model with the best classification accuracy on the hold-out validation set was selected for comparison against the alternative methods in this work.  
	 
	 
	 
	 \subsection{Information Theoretic Learning}
	 The methods previously described are what the author considers as ``traditional" forms of learning.  In this sense, these methods typically rely on low-order statistics to capture prediction error.  Alternatively, Information Theoretic Learning (ITL) approaches have proven effective in a variety of learning applications \cite{Principe2010ITL}.  These methods take advantage of class distributions to compare higher-order statistics and thus provide richer  descriptions of classification error.
	 
	 %% Review of XEnt
	 \subsubsection*{Minimum Cross-Entropy (xEnt)}
	 One such information-theoretic metric is Minimum Cross-Entropy (MinxEnt).  As outlined by Principe in \cite{Principe2010ITL}, MinxEnt is formulated as a constrained minimization of KL-divergence between the label and predicted-value distributions, $p_d$ and $p_y$. 
	 \begin{align}
	 	\min_{p_y}D_{KL}(p_d||p_y) \quad \text{subject to (constraints)}
	 \end{align} 
	 
	 \noindent To implement this comparison, a small amount of Gaussian noise is added to the one-hot encoded label vectors:
	 
	 \begin{align}
	 	d_{new} = d + \mathcal{N}(0,\sigma^2\mathbf{I})
	 \end{align}
	 
	 \noindent where $d\in\mathbb{R}^{10\times1}$, and $\sigma^2$ was arbitrarily chosen as $0.002$.
	 
	 \subsubsection*{ITL Network Architecture}
	 In order to provide a comparison to the alternative methods, multi-layer perceptron networks were implemented and trained with MinxEnt.  The network architectures were designed as $784 \rightarrow 500 \rightarrow 200 \rightarrow k \rightarrow 10$ which is consistent with the autoencoder networks defined earlier in this work.  The value of $k$ was selected to match the bottleneck sizes of the tested autoencoders and an output layer of size 10 was employed to allow for label comparison.  ReLU activations were utilized in each of the hidden layers and a softmax was applied at the output to produce class probability vectors.
	 
	 %% Experiments
	 \subsubsection*{ITL Experiments}
	 As with the previous experiments, networks were trained 5 times each with an initial learning rate of $\eta=0.01$ and updated with Adamax.  Mini-batches of 200 were utilized for 20 epochs.  For each of the sizes of $k$, $[10,25,50,75,100]$, models were trained using a range of bandwidth parameters on the parzen window estimator for xEnt.  Specifically, bandwidths of $\sigma = [3,30,300,300]$ were implemented to allow for broad generalization of effects on classification performance. During test, maximum a-priori with priors on the noisy one-hot encoded label vectors was used to assign hard class labels. Comparison of performance based on bandwidth and value of $k$ is provided in Section \ref{bw_comparison}.
	 


%==================================================================================================
%========================================= Results ================================================
%==================================================================================================
\section{Results} \label{Results}
In this section, results from experimentation are presented.  Comparison of autoencoder bottleneck size is shown, classification performance on auto-encoded data is presented and a comparison of ITL approaches is revealed. 

\subsection{Autoencoder Reconstruction} \label{ae_reconstruction}
Each of the autoencoder  networks tested achieved near-zero MSE loss. This value was negligible, however, as MSE was being computed on values between $[0,1]$.  This inherently makes MSE small.  Instead, reconstruction results of a shoe passed through bottlenecks of 10 and 100, respectively, are shown in Figure \ref{fig:ae_reconstruction}.  As can be observed, the reconstructed image from the wider bottleneck retains more fine detail than the smaller.  This is at the cost of dimensionality, however.

%% Examples of reconstructed images here
\begin{figure}%
	\centering
	\subfloat[]{{\includegraphics[width=4.5cm]{"ae_reconstructed_images/ae_latent_10_img_2"} }}%
	\subfloat[]{{\includegraphics[width=4.5cm]{"ae_reconstructed_images/ae_latent_100_img_1"} }}%
	\caption{Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality 10 (a) and 100 (b).  The images' original dimensionality was 784.}%
	\label{fig:ae_reconstruction}%
\end{figure}


\subsection{Confusion Matrices}

A confusion matrix demonstrates the discrepancies between predicted and true class values for groups of samples.  Essentially, it is a way to measure how accurate a classifier is, while providing insight into how the network confuses samples.  A diagonal matrix signifies zero mis-classifications among all categories.  Confusion matrices are used in this work to provide insight to classification performance.

\subsection{Comparison of Bottleneck Size} \label{classification_accuracy}
This section presents classification accuracies for the top performing baseline CNN and SVMs trained on the autoencoded data.  Table \ref{tab:step1comparison} shows that the baseline CNN achieved the best classification accuracy of 90\%.  The second-best performance was exhibited by the SVM trained on 100D features, which achieved 86\% classification accuracy.  Performance dropped sequentially with dimensionality reduction.  The lowest performing system was the SVM trained on 10D features.  This model obtained 76\% classification accuracy.  Confusion matrices for the baseline CNN, SVM on 100D data and SVM trained on 10D data are shown in Figure \ref{fig:step_1_conf_mat}.
From the figures, it can be observed that the mis-classified classes are fairly consistent between each of the models.  The most-commonly confused classes are shirt, pullover, and coat.
 
 \begin{table}[h]
 	\caption{Classification Accuracies for Different Neural Model/ Classification Systems}
 	\label{tab:step1comparison}
 	\normalsize
 	\begin{tabularx}{\columnwidth}{ |X|X| } 
 		\hline
 		\centering \textbf{Classification Model}  & \textbf{Accuracy} \\
 		\hline
 		\centering Baseline CNN & \textbf{0.90} \\
 		\hline
 		\centering SVM 100D & \underline{0.86} \\
 		\hline
 		\centering SVM 75D & 0.85 \\
 		\hline
 		\centering SVM 50D & 0.84 \\
 		\hline
 		\centering SVM 25D & 0.81 \\
 		\hline
 		\centering SVM 10D & 0.76 \\
 		\hline
 	\end{tabularx}
 \end{table} 

\begin{figure*}[h!]%
	\centering
	\subfloat[]{{\includegraphics[width=6cm]{"cnn_model_parameters/baseline_cnn2_confusion_mat"} }}%
	\subfloat[]{{\includegraphics[width=6cm]{"svm_conf_mat/svm_feature_size_100_default"} }}%
	\subfloat[]{{\includegraphics[width=6cm]{"svm_conf_mat/svm_feature_size_10_default"} }}%
	\caption{Confusion matrix for the baseline CNN (a), SVM trained on data passed through an encoder network with bottleneck dimensionality 100 (b) and SVM trained on data passed through an encoder network with bottleneck dimensionality 10 (c).  The CNN model classified 90\% of the test data correctly while the SVM 100D model classified 86\% correctly and the  SVM 10D classified 76\% correctly.}%
	\label{fig:step_1_conf_mat}%
\end{figure*}

 
\subsection{Comparison of xEnt Kernel Bandwidths} \label{bw_comparison}
This section demonstrates classification accuracies for the networks trained with MinxEnt.  Table \ref{tab:bw_comparison} shows test classification accuracies for the networks trained with different values for the parzen window estimator bandwidth and dimensionality of the smallest layer (before 10D final output).  Addtionally, confusion matrices for the best-performing models for each latent dimensionality are provided by Figures \ref{fig:feature_size_100_bw_300}, \ref{fig:feature_size_75_bw_300}, \ref{fig:feature_size_50_bw_3}, \ref{fig:feature_size_25_bw_300} and \ref{fig:feature_size_10_bw_300}, respectively. From the table, it can be seen that a bandwidth of $\sigma=300$ consistently provided the best results among the parameters compared. The highest performing model was the MLP with a final layer with  100 units and a MinxEnt bandwidth of 300.  Moreover, the confusion matrices show that the networks had similar confusers to the previous experiments.  The shirt, coat, and pullover classes were the most difficult for the networks to discriminate between.

\begin{table}[h!]
	\caption{Classification Accuracies for Varying xEnt Kernel Bandwidths and Latent Dimensionalities}
	\label{tab:bw_comparison}
	\normalsize
	\begin{tabularx}{\columnwidth}{ |X|X|X| } 
		\hline
		\textbf{Latent Dim. $k$}  & \textbf{Bandwidth $\sigma$} & \textbf{Accuracy} \\
		\hline
		100D & 3 & 0.74\\
		\hline
		 & 30 & 0.79\\
		\hline
		 & \textbf{300} & \textbf{0.89}\\
		\hline
		 & 3000 & 0.86\\
		\hline
		75D & 3 & 0.60\\
		\hline
		 & 30 & 0.65\\
		\hline
		 & \textbf{300} & \textbf{0.74}\\
		\hline
		 & 3000 & 0.50\\
		\hline
		50D & \textbf{3} & \textbf{0.88}\\
		\hline
		 & 30 & 0.87\\
		\hline
		 & 300 & 0.78\\
		\hline
		 & 3000 & 0.71\\
		 \hline
		25D & 3 & 0.72\\
		\hline
		& 30 & 0.62\\
		\hline
		& \textbf{300} & \textbf{0.73}\\
		\hline
		& 3000 & 0.52\\
		\hline
		10D & 3 & 0.68\\
		\hline
		& 30 & 0.78\\
		\hline
		& \textbf{300} & \textbf{0.80}\\
		\hline
		& 3000 & 0.66\\
		\hline
	\end{tabularx}
\end{table} 


\begin{center}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.4\textwidth]{"mlp_itl_model_parameters/feature_size_100_bw_300"}
		\caption{Confusion matrix for the MLP with a final dimensionality (before output) of $k=100$ and a xEnt bandwidth of $\sigma=300$.  The model classified 89\% of the test data correctly.}
		\label{fig:feature_size_100_bw_300}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.4\textwidth]{"mlp_itl_model_parameters/feature_size_75_bw_300"}
		\caption{Confusion matrix for the MLP with a final dimensionality (before output) of $k=75$ and a xEnt bandwidth of $\sigma=300$.  The model classified 74\% of the test data correctly.}
		\label{fig:feature_size_75_bw_300}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\textwidth]{"mlp_itl_model_parameters/feature_size_50_bw_3"}
		\caption{Confusion matrix for the MLP with a final dimensionality (before output) of $k=50$ and a xEnt bandwidth of $\sigma=3$.  The model classified 88\% of the test data correctly.}
		\label{fig:feature_size_50_bw_3}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\textwidth]{"mlp_itl_model_parameters/feature_size_25_bw_300"}
		\caption{Confusion matrix for the MLP with a final dimensionality (before output) of $k=25$ and a xEnt bandwidth of $\sigma=300$.  The model classified 73\% of the test data correctly.}
		\label{fig:feature_size_25_bw_300}
	\end{figure}
\end{center}

\begin{center}
	\begin{figure}[th]
		\centering
		\includegraphics[width=0.4\textwidth]{"mlp_itl_model_parameters/feature_size_10_bw_300"}
		\caption{Confusion matrix for the MLP with a final dimensionality (before output) of $k=10$ and a xEnt bandwidth of $\sigma=300$.  The model classified 80\% of the test data correctly.}
		\label{fig:feature_size_10_bw_300}
	\end{figure}
\end{center}



%==================================================================================================
%====================================== Discussion ================================================
%==================================================================================================
\vspace{5cm}
\section{Discussion} \label{Discussion}
In this sections, observations are made on results and insight is given to potential influences.

\subsection{Results and Experimental Design}

%% Comparison of ae reconstruction
The first experimentation performed was a comparison of reconstruction error from various autoencoder bottleneck sizes.  Because the image values were normalized between $[0,1]$ when converting to a tensor, and the outputs were passed through a sigmoid activation to scale them appropriately, all MSE values were very low.  This  made it difficult to evaluate performance quantitatively.  However, when plotting images passed through the various networks (as shown in Figure \ref{fig:ae_reconstruction}), a clear trend could be seen.  Specifically, as the bottleneck layer's dimensionality decreased the reconstructed image became less accurate (compared to the original).  This intuitively makes sense, as there is a greater loss of information through smaller bottleneck sizes.  The goal would then be to use the bottleneck layer with the smallest dimensionality which still preserved discriminative qualities between the classes. When tacking on a support vector machine, a clear correlation between bottleneck dimensionality and classification accuracy can be seen.  They highest performing AE + SVM was the model which took the bottle neck features of size 100.  The performance dropped sequentially with a decrease in bottleneck size.  Again, this result is intuitive and, in practice, one would use the smallest feature dimensionality which still allowed the model to achieve a desired level of performance.

%% Comparison of ITL bandwidth
Next, multilayer perceptrons  of the same size to the encoder networks were employed and trained with MinxEnt.  When comparing classification performance over a range bandwidth values, the sensitivity to this parameter in terms of classification accuracy can be observed. The bandwidths of $\sigma=3$ and $\sigma=3000$ consistently resulted in the lowest  level of performance, while a $\sigma=300$ typically provided the best.  This suggests that, for this dataset, the optimal bandwidth value might be somewhere in the magnitude of the hundreds.  However, performance due to bandwidth, which effects both the magnitude and granularity of the error, might not be well represented in this experimentation.  Error could have resulted from the training time of the networks, which was fixed at 20 epochs for every model.  A smaller bandwidth may have taken more epochs of training to reach a minimum (which may have been lower than what was achieved).


%% Comparison of classifiers
When comparing the CNN, AE + SVM, and ITL MLP, the differences in performance can be analyzed.  The baseline CNN model obtained the best performance among the models tested with an overall classification accuracy of 90\%.  This was followed in suit by the ITL MLP with a final 100D layer and a bandwidth of $\sigma=300$ which achieved 89\% accuracy and the ITL MLP with a final 50D layer and a bandwidth of $\sigma=3$ which achieved 88\% accuracy.  The best performing AE + SVM model was the one with a bottleneck size of 100.  This classification system obtained 86\% classification accuracy on the hold-out test set.  However, these results match the author's intuition.  To begin, autoencoders are not designed to preserve class discriminability in the latent space.  They specifically attempt ot compress the data into  representations which capture the highest level of variation in the dataset.  On images of faces, for example, latent features might represent (presumably) color, pose, eye shape, lighting variations, etc.  While these features might provide appropriate features for class discrimination, there is no guarantee they will meet this objective.  Therefore, one would expect the AE + SVM networks to achieve lesser classification accuracy than the alternatives compared in this work.


%% autoencoder only captures variation and may not be best for discrimination, whereas CNN features were learned with classifcaiton in mind

%% Best ITL without convolution was as good as baseline cnn


%% potentially suboptimal parameters in all experimentation
It should be noted that, due to time limitations, hyperparameters for each method were chosen only to provide a rough generalization of performance over a wide range of parameter values.  Hyperparameters were not necessarily tuned in any scenario, which could mis-lead results concerning the overall ``best" algorithm.  If given more time, a more thorough parameter search would be conducted in attempt to discover the optimal models for comparison.

%% cross-validation
In this work, models were only trained five times each, and the best performing model on validation data was analyzed further.  It would have, however, been more insightful to the stability of the methods to run an extensive cross-validation analysis.  This could provide bounds on performance variations due to training, and could highlight the robustness of particular methods which may not have been captured by this set of experiments.

%% run time
It should also be noted that run-time was not compared in this work.  This was due to the fact that the author was switching between machines when performing this experimentation, which resulted in highly-varying run-times for the same algorithm.  For clarity, this should not be done when demonstrating performance of methods, but it was sufficient for this work. 

\subsection{Potential Improvements}
In addition to a more thorough study of optimal parameter selection, two methods which could potentially aid with overall system performance might be to include an additional form of pre/post-processing to improve the confused classes or to train the model with a type of rank loss.  It terms of pre/post-processing, additional models could be trained to learn explicit differences between the coat, shirt and  pullover classes, and between the top and dress classes.  This could done before-hand by learning alternative  features representations for these samples before being passed to the main network, or after-the-fact with additional classification on any labels predicted to be in those difficult classes.  This could even be implemented in an ensemble approach to (hopefully) improve classification performance on the most-likely confused classes.  Another alternative  would be to employ a form of rank-loss, such as contrastive or triplet.  In this scenario, additional constraints could be applied to the objective function to enforce inter-class separability between disparate classes and promote intra-class compactness.  These approaches utilize pairs or triplets of similar and  dissimilar data samples to embed samples  in close/far relations according to a predefined metric.  The author believes this could be applied  in conjunction with the cost functions utilized in this work to improve results on the difficult classes.




%==================================================================================================
%===================================== Conclusions ================================================
%==================================================================================================
\section{Conclusions} \label{Conclusions}
Classification of Fashion-MNIST images using information theoretic and non-information theoretic cost functions was investigated in this work.  Dimensionality reduction was performed using autoencoder neural networks and classification was performed using support vector machines. Additionally, multilayer perceptrons were trained using minimum cross-entropy.  Both classification frameworks were compared to a baseline CNN model.  The baseline achieved the highest classification accuracy amongst the models compared with a 90\% correct classification rate.  Specific implementations of the AE + SVM and ITL MLP networks obtained comparable results.\\
\indent The author believes that the primary differences in performance between the models comes from their methods of feature learning.  Autoencoders learn latent representations to preserve variation in the data, and do not necessarily learn latent features which are optimized for  discrimination.  However, both the CNN and ITL MLP networks are trained with classification in mind and thus have an advantage over the autoencoded features.  Moreover, as can be observed from the literature, convolutional features seem to represent image qualities better than traditional MLPs. However, the ITL MLP was able to obtain comparable results to the CNN despite this.  This might suggest the usefulness of the information theoretic cost function in improving classification results.  Performance might be improved more using a convolutional network with an  information theoretic cost function.  \\
\indent
Future research endeavors toward this topic include pre/post-processing or ensembling to improve classification results on the difficult-to-classify classes and investigating the incorporation of rank-loss through metric embedding to provide improved separability of classes in the feature space.

%==================================================================================================
%==================================== Bibliography ================================================
%==================================================================================================

\section*{Honor Statement}
\noindent
* I confirm that this assignment is my own work, it is not copied from any other person's work (published or unpublished), and has not been previously submitted for assessment either at University of Florida or elsewhere.

 \begin{figure}[h!]
 	\centering
 	\includegraphics[width=0.20\textwidth]{"Signature"}
 \end{figure}

\bibliography{Project2}
\bibliographystyle{IEEEtran}

\end{document}
