\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage[round]{natbib}
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lettrine}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{subfig}





\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}



\graphicspath{{"C:/Users/Conma/Documents/2019_08_Principe_Deep_Learning/Homework/Project02/Report/Images/"} {"/media/edrive/2019_08_Principe_Deep_Learning/Homework/Project02/Report/Images/"}}

\begin{document}

\title{Project 2: A Comparative Study Between Traditional and Information Theoretic Neural Network Models for Fashion-MNIST Classification}
\author{\IEEEauthorblockN{Connor McCurley}
\IEEEauthorblockA{\textit{Deep Learning, Fall 2019} \\
\textit{University of Florida}\\
Gainesville, FL, USA 32611 \\
Email: cmccurley@ufl.edu}

}

\maketitle


\begin{abstract}
	
\end{abstract} 

\begin{IEEEkeywords}
Fashion MNIST, Autoencoder, Information Theoretic Learning, Support Vector Machine
\end{IEEEkeywords}

%==================================================================================================
%====================================  Introduction ===============================================
%==================================================================================================
\section{Introduction} 

%\lettrine{A}{utonomous} image classification is a challenging problem which offers potential for significant advancement in the areas of biometrics, biology, medical diagnosis, security, and more \cite{Prasad2015Review,Lu2007Review}.  This paper focuses on the use of dimensionality reduction/ manifold learning in conjunction with multi-layer perceptron artificial neural networks to automatically classify clothing items from the well-known Fashion MNIST dataset \cite{Xiao2017FashionMNIST}.  \\
%\indent A wide variety of approaches have been taken in attempt to solve detection and classification problems in imagery.  \cite{Mensink2013KNN} used dissimilarity-based classifiers along with metric learning to dually drive samples toward their respective class representatives while also enforcing separation between classes.  \cite{Sanchez2011svm,Lin2011svm} utilized vector embeddings with linear support vector machines to discriminate between low-dimensional image representations.  The work in \cite{Shao2019DictionaryLearning} found sparse weighted combinations of dictionary atoms to accurately reconstruct images where specific bases equated to the various  classes.  The authors of \cite{Timofte2013NaiveBayes} utilized statistical properties to match samples to generating distributions.  The work in \cite{Swaroop2016TemplateMatching} employed traditional template matching to locate objects or compositions in imagery.  The review in \cite{Driss2017MLPandCNN} demonstrated that expansive uses of artificial neural networks in image classification.  This, of course, is just a small sample of image classification techniques.  The reviews in \cite{Prasad2015Review,Lu2007Review} elaborate extensively on the myriad of methods.  A commonality among all of the discussed methods is that they suffer from high-dimensionality.  Because of this fact, this work explores the use of dimensionality reduction as a preprocessing procedure for classification with fully-connect multi-layer perceptrons. \\
%\indent The remainder of this paper is organized as follows.  Section \ref{Methodology} describes the methodology used to perform dimensionality reduction and classification with multilayer perceptrons.  Classification results are presented in Section \ref{Results}.  Practical insights to results are given in Section \ref{Discussion}.  Finally, Section \ref{Conclusions} reveals concluding remarks and discusses future lines of research.

%==================================================================================================
%====================================  Methodology ================================================
%==================================================================================================

\section{Methodology} \label{Methodology}
	This section describes the methodology implemented in this work.  Analysis of the data is performed, dimensionality reduction and classification procedures are described, various network architectures under analysis are elaborated on and experimental procedures are outlined.

	%% Data analysis
	\subsection{Data Analysis}
	 The following data analysis was originally described in \cite{McCurley2019PrincipeProject1}, but is pertinent to this work and is thus re-analyzed here.  The data was plotted as shown in Figure \ref{fig:samples} to gain an understanding of the format.  Each sample in the Fashion-MNIST dataset is a 28x28, gray-scale image of a clothing item belonging to one of ten classes \cite{Xiao2017FashionMNIST}. This translates to 784 length feature vectors with values ranging between 0-255. There were exactly 60000 training images included in the  training dataset and 10000 which were held-out for test.  The 60000 samples were later sub-divided in the experimentation for cross-validation.  As with Project 1, dimensionality reduction(DR) was incorporated to reduce data complexity.  This DR was employed through the use of stacked autocoder neural networks (SAE).  Elaboration on the SAE networks is provided in the following section.
	
	%% Fashion MNIST data samples
	 \begin{center}
	 	\begin{figure*}[h]
	 		\centering
	 		\includegraphics[width=0.8\textwidth]{"samples/samples"}
	 		\caption{Samples from the Fashion-MNIST dataset. One sample from each class was randomly chosen for visualization.  The gray-scale images are size 28x28, each representing an article of clothing.}
	 		\label{fig:samples}
	 	\end{figure*}
	 \end{center}

	%% Autoencoder
	 \subsection{Autoencoder} 
	 \subsubsection*{Description}
	 An autoencoder is a specific taxonomy of artificial neural network which learns how to  compress and de-compress representations of data \cite{Haykin2009NeuralNetworks,Goodfellow2016DeepLearning}. The first half of an autoencoder typically performs dimensionality reduction through non-linear transformations until the middle layer, known as the \textit{bottleneck} or \textit{latent} layer.  The goal of the \textit{encoder} is to learn efficient representations of the factors which govern variation in the data, or in terms of compression, \textit{codes} which can be used to reconstruct the input  data with high accuracy. The second half of an SAE (which is called the \textit{decoder}) projects the data back into its original dimensionality in attempt to reconstruct the original sample.(See Figure \ref{fig:autoencoder}.) Reconstruction loss is between the input and output is used to update the network's parameters.  In practice, samples can be passed through the encoder to perform dimensionality reduction. 
	 
	 %% Autoencoder network
	 \begin{center}
	  	\begin{figure}[t]
	  		\centering
	  		\includegraphics[width=0.4\textwidth]{"autoencoder"}
	  		\caption{Block diagram of an autoencoder neural network.  The layers consecutively reduce dimensionality until the middle (bottleneck) layer.  The second half of the network transforms the data back to the size of the input.  The desired value of the network is the original image.}
	  		\label{fig:autoencoder}
	  	\end{figure}
	  \end{center}
	 
	 %% Examples of reconstructed images here
  	\begin{figure}%
  		\centering
  		\subfloat[]{{\includegraphics[width=5cm]{"ae_reconstructed_images/ae_latent_10_img_2"} }}%
  		\qquad
  		\subfloat[]{{\includegraphics[width=5cm]{"ae_reconstructed_images/ae_latent_100_img_1"} }}%
  		\caption{Reconstructed images of a shoe after passing through an SAE with bottleneck dimensionality 10 (a) and 100 (b).  The images' original dimensionality was 784.}%
  		\label{fig:example}%
  	\end{figure}

  
  	%% Architecture parameters
  	\subsubsection*{SAE Architecture}
  	The SAE architecture tested in this work consisted of 5 hidden layers, along with the input and output.  The layers were selected as $784 \rightarrow 500 \rightarrow 200 \rightarrow k \rightarrow 200 \rightarrow 500 \rightarrow 784$, where $k$ is the arbitrarily chosen dimensionality of the bottleneck.  In this work, $k$ was tested at $[10,25,50,75,100]$ in order to provided a reasonable comparison of performance changes resulting from dimensionality reduction.  ReLU activation functions were used to apply nonlinearity.  A sigmoid activation, however, was used at the output layer to enforce image value constraints between $[0-1]$.  This was done because the images were normalized between $[0-1]$ before passing through the network.  An initial learning rate of $\eta=0.01$ was selected, and was updated using the Adamax optimizer through training.
  	
  	%% Description of experiments
  	\subsubsection*{SAE Experiments}
  	The SAE network was trained for 20 epochs using mini-batch sizes of 200 samples.  The bottleneck layer's dimensionality was varied between $[10,25,50,75,100]$.  Each network configuration was trained 5 times, and the model which provided the lowest reconstruction  Mean-Squared Error (MSE) on the hold-out validation set was selected for further use in classification.  Results are shown in section \ref{ae_reconstruction}.
  	
	 %% Support vector Machines
	 \subsection{Support Vector Machines}
	 \subsubsection*{Description}
	 A Support Vector Machine (SVM) is a specific class of sparse kernel machines whose objective  is to learn a decision boundary which can adequately discriminate between classes in a high-dimensional space \cite{Lin2011svm,Sanchez2011svm}.  Because of its sparsity constraints, a SVMs' predictions rely only on a subset of the training data known as \textit{support vectors}. By design, support vectors tend to be examples in the training data which lie closest to the decision boundary, and are thus the most prone to mis-classification. A primary difference between SVM and methods relying on Information Theory is that vanilla SVM classification predictions are not probabilistic \cite{Murphy2012Textbook}.  In other words, hard labels are assigned which do not capture the uncertainty of the prediction results.  In this work, SVMs were trained on the data passed through the selected autoencoders.  A comparison of classification performance on the various sizes of features is provided in section \ref{classification_accuracy}.
	 
	 \subsubsection*{Parameters}
	 Non-linear support vector machines were used as the classifiers in this work. The necessary hyperparameters were the type of mapping kernel,  kernel parameters, and a regularization (slack) parameter.  A radial-basis function was arbitrarily chosen as the mapping function.  Silverman's rule was used to provided a reasonable range for the kernel bandwidth. The regularization parameter was set to the Python Scikitlearn's default value of $C=1$.  These parameter choices provided reasonable results for comparison.  More parameter variations for the SVM were not tested due to time limitations.
	 
	 \subsubsection*{Experiments}
	 SVMs were employed using Scikitlearn's SVM SVC package.  The classifiers were trained using one-versus-one training.  At test, a sample was applied to the classifier ensemble and the most-likely label was provided to the sample.  Data was passed through each of the top 5 trained autoencoders, and a single SVM was trained to provide label predictions for each input feature dimensionality, $[10,25,50,75,100]$.  Classification performance is presented in section \ref{classification_accuracy}. 
	 
	 
	 \subsection{Baseline CNN}
	 
	 %% Architecture
	 
	 %% Training
	 
	 
	 \subsection{Information Theoretic Learning (Minimum Cross-Entropy)}
	 
	 %% Review of XEnt
	 
	 %% Labels
	 
	 %% Networks
	 
	 %% Experiments
	 
	 
	 
\subsection{Experiments} \label{Experiments}
Experiments 

%\begin{enumerate}
%	\item \textit{Baseline:} Model trained on the original data (described in section \ref{Methodology}).
%	\item \textit{PCA 100:} Principal vectors estimated from the training set were used to project the data down to 100 dimensions, thus retaining 91.1$\%$ of the original variance.  The model was the same as the baseline excluding the input layer.
%	\item \textit{PCA 400:} Principal vectors estimated from the training set were used to project the data down to 400 dimensions, thus retaining 98.5$\%$ of the original variance.  The model was the same as the baseline excluding the input layer.
%	\item \textit{UMAP 100:} UMAP was used to project the data down to 100 dimensions.  15 neighbors were used in the local neighborhood and the minimum distance of points in the latent space was constrained to 0.1, which was measured by Euclidean distance.  The model was the same as the baseline excluding the input layer.
%	\item \textit{UMAP 400:} UMAP was used to project the data down to 400 dimensions.  15 neighbors were used in the local neighborhood and the minimum distance of points in the latent space was constrained to 0.1, which was measured by Euclidean distance.  The model was the same as the baseline excluding the input layer.
%	\item \textit{AE 100:} An additional multi-layer perceptron was first trained in the form of an autoencoder.  This MLP demonstrated layers as (784-500-400-150-100) in the encoder with the reverse in the decoder.  A hyperbolic tangent activation was applied at the output layer.  The network was trained with mean-square error loss and updated with Adamax. The original dataset was passed through the encoder to reduce dimensionality. The model was the same as the baseline excluding the input layer.
%	\item \textit{AE 400:}  An additional multi-layer perceptron was first trained in the form of an autoencoder.  This MLP demonstrated layers as (784-500-400) in the encoder with the reverse in the decoder.  A hyperbolic tangent activation was applied at the output layer.  The network was trained with mean-square error loss and updated with Adamax. The original dataset was passed through the encoder to reduce dimensionality. The model was the same as the baseline excluding the input layer.
%\end{enumerate}





%==================================================================================================
%========================================= Results ================================================
%==================================================================================================
\section{Results} \label{Results}

\subsection{Autoencoder Reconstruction} \label{ae_reconstruction}

%% Table of loss values

%% Examples  of reconstructed images

\subsection{Confusion Matrices}

In this section, classification results for each experimental method tested are presented in the form of confusion matrices.  A confusion matrix demonstrates the discrepancies between predicted and true class values for groups of samples.  Essentially, it is a way to measure how accurate a classifier is, while providing insight into how the network confuses samples.  A diagonal matrix signifies zero mis-classifications among all categories.

\subsection{Comparison of Cost Functions} \label{classification_accuracy}


\begin{table}[h!]
	\caption{Classification Accuracies for Different Neural Model/ Classification Systems}
	\label{tab:step1comparison}
	\normalsize
		\begin{tabularx}{\columnwidth}{ |X|X| } 
			\hline
			\centering \textbf{Classification Model}  & \textbf{Accuracy} \\
			\hline
			\centering Baseline CNN & 0.90 \\
			\hline
			\centering SVM 100D & 0.86 \\
			\hline
			\centering SVM 75D & 0.85 \\
			\hline
			\centering SVM 50D & 0.84 \\
			\hline
			\centering SVM 25D & 0.81 \\
			\hline
			\centering SVM 10D & 0.76 \\
			\hline
		\end{tabularx}
\end{table} 
 
\subsection{Comparison of XEnt Kernel Bandwidths}

\begin{table}[h!]
	\caption{Classification Accuracies for Different XEnt Kernel Bandwidths}
	\label{tab:bw_comparison}
	\normalsize
	\begin{tabularx}{\columnwidth}{ |X|X|X| } 
		\hline
		\centering \textbf{Bottleneck Size}  & \textbf{Bandwidth} & \textbf{Accuracy} \\
		\hline
		\centering Baseline CNN & 0.90 \\
		\hline
		\centering SVM 100D & 0.86 \\
		\hline
		\centering SVM 75D & 0.85 \\
		\hline
		\centering SVM 50D & 0.84 \\
		\hline
		\centering SVM 25D & 0.81 \\
		\hline
		\centering SVM 10D & 0.76 \\
		\hline
	\end{tabularx}
\end{table} 

%==================================================================================================
%====================================== Discussion ================================================
%==================================================================================================
\vspace{5cm}
\section{Discussion} \label{Discussion}
In this sections, observations are made on results and insight is given to potential influences.

%% Comparison of ae reconstruction

%% Comparison of classifiers

%% autoencoder only captures variation and may not be best for discrimination, whereas CNN features were learned with classifcaiton in mind


%% Comparison of ITL bandwidth


%% potentially suboptimal parameters in all experimentation

\subsection{Results}

% \begin{center}
% 	\begin{figure}[t]
% 		\centering
% 		\includegraphics[width=0.5\textwidth]{"models/embedding_pca"}
% 		\caption{Fashion MNIST emmbedding of the training set via PCA}
% 		\label{fig:pca_embedding}
% 	\end{figure}
% \end{center}

% \begin{center}
% 	\begin{figure}[t]
% 		\centering
% 		\includegraphics[width=0.5\textwidth]{"models/embedding_umap"}
% 		\caption{Fashion MNIST embedding of the training set via UMAP}
% 		\label{fig:umap_embedding}
% 	\end{figure}
% \end{center}

%  \begin{center}
% 	\begin{figure}[t]
% 		\centering
% 		\includegraphics[width=0.5\textwidth]{"hist_after_umap_100"}
% 		\caption{Histograms of one versus all Euclidean distances after projecting to 100 dimensions with UMAP for top left): Pullover, top right): Dress, bottom left): Sandal, and bottom right): Trouser classes.  The red bars represent Euclidean distances for samples of the given class to their mean.  Blue represents the distance of every other training sample to the same mean.}
% 		\label{fig:hist_after_umap_100}
% 	\end{figure}
% \end{center}

\subsection{Potential Improvements}
In 




%==================================================================================================
%===================================== Conclusions ================================================
%==================================================================================================
\section{Conclusions} \label{Conclusions}
Three \\
\indent
Future research endeavors toward this topic include, 

% \begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/base_model_confusion_mat"}
% 		\caption{Confusion matrix for the base MLP on 10000 blind test images.}
% 		\label{fig:base_model_confusion_mat}
% 	\end{figure}
% \end{center}
% \begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/pca_100_model_confusion_mat"}
% 		\caption{Confusion matrix for the PCA 100 model on 10000 blind test images.}
% 		\label{fig:pca_100_model_confusion_mat}
% 	\end{figure}
% \end{center}
% \begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/pca_400_model_confusion_mat"}
% 		\caption{Confusion matrix for the PCA 400 model on 10000 blind test images.}
% 		\label{fig:pca_400_model_confusion_mat}
% 	\end{figure}
% \end{center}
% \begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/umap_100_model_confusion_mat"}
% 		\caption{Confusion matrix for the UMAP 100 model on 10000 blind test images.}
% 		\label{fig:umap_100_model_confusion_mat}
% 	\end{figure}
% \end{center}\begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/umap_400_model_confusion_mat"}
% 		\caption{Confusion matrix for the UMAP 400 model on 10000 blind test images.}
% 		\label{fig:umap_400_model_confusion_mat}
% 	\end{figure}
% \end{center}
% \begin{center}
% 	\begin{figure}[h!]
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/auto_100_model_confusion_mat"}
% 		\caption{Confusion matrix for the autoencoder 100 model on 10000 blind test images.}
% 		\label{fig:auto_100_model_confusion_mat}
% 	\end{figure}
% \end{center}
% \begin{center}
% 	\begin{figure}[h!]\textbf{}
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{"models/auto_400_model_confusion_mat"}
% 		\caption{Confusion matrix for the autoencoder 400 model on 10000 blind test images.}
% 		\label{fig:auto_400_model_confusion_mat}
% 	\end{figure}
% \end{center} 


%==================================================================================================
%==================================== Bibliography ================================================
%==================================================================================================

\section*{Honor Statement}
\noindent
* I confirm that this assignment is my own work, it is not copied from any other person's work (published or unpublished), and has not been previously submitted for assessment either at University of Florida or elsewhere.

% \begin{figure}[h!]
% 	\centering
% 	\includegraphics[width=0.20\textwidth]{"Signature"}
% \end{figure}

\bibliography{Project2}
\bibliographystyle{IEEEtran}
%\bibliographystyle{plainnat}

\end{document}
