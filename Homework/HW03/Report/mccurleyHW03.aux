\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of a TDNN with a window size of 7. A 1D convolutional layer creates a feature vector for the input sequence before passing it along to a hidden layer and a subsequent output layer. A single label is predicted for the entire sequence.\relax }}{1}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tdnn7_architecture}{{1}{1}{Architecture of a TDNN with a window size of 7. A 1D convolutional layer creates a feature vector for the input sequence before passing it along to a hidden layer and a subsequent output layer. A single label is predicted for the entire sequence.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architecture of a TDNN with a window size of 20. A 1D convolutional layer creates a feature vector for the input sequence before passing it along to a hidden layer and a subsequent output layer. A single label is predicted for the entire sequence.\relax }}{1}{figure.caption.4}}
\newlabel{fig:tdnn20_architecture}{{2}{1}{Architecture of a TDNN with a window size of 20. A 1D convolutional layer creates a feature vector for the input sequence before passing it along to a hidden layer and a subsequent output layer. A single label is predicted for the entire sequence.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curves for the TDNN with a window size of 7. The validation loss begins to increase at approximately training epoch 1200.\relax }}{2}{figure.caption.5}}
\newlabel{fig:tdnn7_learning_curve_lr_point001}{{3}{2}{Learning curves for the TDNN with a window size of 7. The validation loss begins to increase at approximately training epoch 1200.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curves for the TDNN with a window size of 20. The validation loss begins to increase at approximately training epoch 900.\relax }}{2}{figure.caption.6}}
\newlabel{fig:tdnn20_learning_curve_lr_point001}{{4}{2}{Learning curves for the TDNN with a window size of 20. The validation loss begins to increase at approximately training epoch 900.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrix for the TDNN with a window size of 7. Out of the 500 test sequences, this particular network mis-classified 130. 26 in-grammar sequences were predicted as out of grammar, while 104 out of grammar sequences were predicted in-grammar.\relax }}{3}{figure.caption.7}}
\newlabel{fig:tdnn7_confusion_matrix_lr_point001}{{5}{3}{Confusion matrix for the TDNN with a window size of 7. Out of the 500 test sequences, this particular network mis-classified 130. 26 in-grammar sequences were predicted as out of grammar, while 104 out of grammar sequences were predicted in-grammar.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Confusion matrix for the TDNN with a window size of 20. Out of the 500 test sequences, this particular network mis-classified 110. 1 in-grammar sequence was predicted as out of grammar, while 109 out of grammar sequences were predicted as in-grammar.\relax }}{3}{figure.caption.8}}
\newlabel{fig:tdnn20_confusion_matrix_lr_point001}{{6}{3}{Confusion matrix for the TDNN with a window size of 20. Out of the 500 test sequences, this particular network mis-classified 110. 1 in-grammar sequence was predicted as out of grammar, while 109 out of grammar sequences were predicted as in-grammar.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curves for the RNN. The validation loss begins to increase at approximately training epoch 1200.\relax }}{4}{figure.caption.10}}
\newlabel{fig:rnn_learning_curve_lr_point001}{{7}{4}{Learning curves for the RNN. The validation loss begins to increase at approximately training epoch 1200.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confusion matrix for the RNN. Out of the 500 test sequences, this particular network mis-classified 41. 0 in-grammar sequences were predicted as out of grammar, while 41 out of grammar sequences were predicted in-grammar.\relax }}{4}{figure.caption.11}}
\newlabel{fig:rnn_confusion_matrix_lr_point001}{{8}{4}{Confusion matrix for the RNN. Out of the 500 test sequences, this particular network mis-classified 41. 0 in-grammar sequences were predicted as out of grammar, while 41 out of grammar sequences were predicted in-grammar.\relax }{figure.caption.11}{}}
\bibcite{Principe}{1}
\bibcite{Principe}{2}
