\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of the baseline MLP. The network defines a single hidden layer MLP with 14 units in the hidden layer and 6 neurons in the output layer. ReLU activations were applied at the hidden units, while a softmax was used at the output to provide class score probabilities. \relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Q1_architecture}{{1}{1}{Architecture of the baseline MLP. The network defines a single hidden layer MLP with 14 units in the hidden layer and 6 neurons in the output layer. ReLU activations were applied at the hidden units, while a softmax was used at the output to provide class score probabilities. \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves for the ``best" instance of the model described in Figure \ref  {fig:Q1_architecture}. At 200 epochs the cross-entropy loss on the training dataset is approximately 0.2, while validation loss is around 0.65. After 200 batch epochs, the model begins to overfit the training data.\relax }}{2}{figure.caption.5}\protected@file@percent }
\newlabel{fig:Q1_learning_curve}{{2}{2}{Learning curves for the ``best" instance of the model described in Figure \ref {fig:Q1_architecture}. At 200 epochs the cross-entropy loss on the training dataset is approximately 0.2, while validation loss is around 0.65. After 200 batch epochs, the model begins to overfit the training data.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Confusion matrix for testing on patient 2. It can be observed from the figure that sleep stages 2 and 4, as well as the awake state, were generally classified with low error. Stage 1 was often predicted as stage 4 or awake and stage 5 was sometimes mis-judged as state 4. The cross entropy loss on this test set was calculated as 0.96.\relax }}{2}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Q1_confusion_matrix}{{3}{2}{Confusion matrix for testing on patient 2. It can be observed from the figure that sleep stages 2 and 4, as well as the awake state, were generally classified with low error. Stage 1 was often predicted as stage 4 or awake and stage 5 was sometimes mis-judged as state 4. The cross entropy loss on this test set was calculated as 0.96.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test loss for a single hidden layer MLP with 14 units in the hidden layer. Cross-entropy loss on a sequestered test set is shown as well as the number of epochs for varying learning rates.\relax }}{3}{table.caption.10}\protected@file@percent }
\newlabel{tab:Q2_learning_rate_variation_14}{{1}{3}{Test loss for a single hidden layer MLP with 14 units in the hidden layer. Cross-entropy loss on a sequestered test set is shown as well as the number of epochs for varying learning rates.\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test loss for a single hidden layer MLP with 40 units in the hidden layer. Cross-entropy loss on a sequestered test set is shown as well as the number of epochs for varying learning rates.\relax }}{3}{table.caption.11}\protected@file@percent }
\newlabel{tab:Q2_learning_rate_variation_40}{{2}{3}{Test loss for a single hidden layer MLP with 40 units in the hidden layer. Cross-entropy loss on a sequestered test set is shown as well as the number of epochs for varying learning rates.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Test loss and number of training epochs as a function of the number of hidden layers. Each hidden layer contained 10 neurons. The learning rate was fixed to $\eta =0.01$.\relax }}{4}{table.caption.13}\protected@file@percent }
\newlabel{tab:Q2_number_hidden_layer_variation}{{3}{4}{Test loss and number of training epochs as a function of the number of hidden layers. Each hidden layer contained 10 neurons. The learning rate was fixed to $\eta =0.01$.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Test loss and number of training epochs as a function of the number of units in a 2 hidden layer MLP. The learning rate was fixed to $\eta =0.01$.\relax }}{4}{table.caption.15}\protected@file@percent }
\newlabel{tab:Q2_number_hidden_units_variation}{{4}{4}{Test loss and number of training epochs as a function of the number of units in a 2 hidden layer MLP. The learning rate was fixed to $\eta =0.01$.\relax }{table.caption.15}{}}
\bibcite{Principe}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curves for the ``best model" chosen during 5-fold cross-validation. Validation loss begins to increase after approximately 3500 training epochs.\relax }}{5}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Q3_learning_curve}{{4}{5}{Learning curves for the ``best model" chosen during 5-fold cross-validation. Validation loss begins to increase after approximately 3500 training epochs.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Confusion matrix for the ``best model" chosen during 5-fold cross-validation. The total cross-entropy loss evaluated to 0.62 for the combined test set.\relax }}{5}{figure.caption.18}\protected@file@percent }
\newlabel{fig:Q3_confusion_matrix}{{5}{5}{Confusion matrix for the ``best model" chosen during 5-fold cross-validation. The total cross-entropy loss evaluated to 0.62 for the combined test set.\relax }{figure.caption.18}{}}
